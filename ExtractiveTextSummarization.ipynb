{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566},{"sourceId":10927526,"sourceType":"datasetVersion","datasetId":6794086},{"sourceId":11025805,"sourceType":"datasetVersion","datasetId":6866109},{"sourceId":11025837,"sourceType":"datasetVersion","datasetId":6866135},{"sourceId":11097726,"sourceType":"datasetVersion","datasetId":6917999},{"sourceId":11097979,"sourceType":"datasetVersion","datasetId":6918135},{"sourceId":11101093,"sourceType":"datasetVersion","datasetId":6920226},{"sourceId":11103041,"sourceType":"datasetVersion","datasetId":6921592},{"sourceId":11103098,"sourceType":"datasetVersion","datasetId":6921636},{"sourceId":11103197,"sourceType":"datasetVersion","datasetId":6921710},{"sourceId":11103251,"sourceType":"datasetVersion","datasetId":6921751},{"sourceId":11111482,"sourceType":"datasetVersion","datasetId":6927567},{"sourceId":11261510,"sourceType":"datasetVersion","datasetId":7038592},{"sourceId":11261793,"sourceType":"datasetVersion","datasetId":7038807},{"sourceId":11261816,"sourceType":"datasetVersion","datasetId":7038827},{"sourceId":11274321,"sourceType":"datasetVersion","datasetId":7048132},{"sourceId":11307022,"sourceType":"datasetVersion","datasetId":7071320},{"sourceId":11362239,"sourceType":"datasetVersion","datasetId":7111534},{"sourceId":11362436,"sourceType":"datasetVersion","datasetId":7111671},{"sourceId":11363753,"sourceType":"datasetVersion","datasetId":7112696},{"sourceId":11374685,"sourceType":"datasetVersion","datasetId":7121132},{"sourceId":11375563,"sourceType":"datasetVersion","datasetId":7121788},{"sourceId":11384099,"sourceType":"datasetVersion","datasetId":7128273},{"sourceId":11384207,"sourceType":"datasetVersion","datasetId":7128351},{"sourceId":11384287,"sourceType":"datasetVersion","datasetId":7128412},{"sourceId":11459338,"sourceType":"datasetVersion","datasetId":7180293},{"sourceId":11459414,"sourceType":"datasetVersion","datasetId":7180345},{"sourceId":11459443,"sourceType":"datasetVersion","datasetId":7180362},{"sourceId":11482899,"sourceType":"datasetVersion","datasetId":7196933}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Summarization","metadata":{}},{"cell_type":"code","source":"print(\"Hello World\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T03:52:30.536563Z","iopub.execute_input":"2025-04-21T03:52:30.536761Z","iopub.status.idle":"2025-04-21T03:52:32.227863Z","shell.execute_reply.started":"2025-04-21T03:52:30.536741Z","shell.execute_reply":"2025-04-21T03:52:32.226976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install textacy contractions keras swifter faiss-gpu rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:20:44.320255Z","iopub.execute_input":"2025-04-21T10:20:44.320531Z","iopub.status.idle":"2025-04-21T10:20:58.747864Z","shell.execute_reply.started":"2025-04-21T10:20:44.320510Z","shell.execute_reply":"2025-04-21T10:20:58.746827Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading dataset and basic visualisation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport gensim\nimport torch\nfrom tqdm import tqdm\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom gensim.models import KeyedVectors\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:22:40.844121Z","iopub.execute_input":"2025-04-21T10:22:40.844471Z","iopub.status.idle":"2025-04-21T10:22:57.618680Z","shell.execute_reply.started":"2025-04-21T10:22:40.844443Z","shell.execute_reply":"2025-04-21T10:22:57.617783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:55:05.939251Z","iopub.execute_input":"2025-03-13T06:55:05.939630Z","iopub.status.idle":"2025-03-13T06:55:24.092011Z","shell.execute_reply.started":"2025-03-13T06:55:05.939602Z","shell.execute_reply":"2025-03-13T06:55:24.091016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:51:59.319804Z","iopub.execute_input":"2025-03-12T16:51:59.320159Z","iopub.status.idle":"2025-03-12T16:51:59.330889Z","shell.execute_reply.started":"2025-03-12T16:51:59.320130Z","shell.execute_reply":"2025-03-12T16:51:59.329727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:57:47.618177Z","iopub.execute_input":"2025-03-13T06:57:47.619943Z","iopub.status.idle":"2025-03-13T06:57:47.631127Z","shell.execute_reply.started":"2025-03-13T06:57:47.619862Z","shell.execute_reply":"2025-03-13T06:57:47.629634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:52:03.394127Z","iopub.execute_input":"2025-03-12T16:52:03.394476Z","iopub.status.idle":"2025-03-12T16:52:03.529817Z","shell.execute_reply.started":"2025-03-12T16:52:03.394444Z","shell.execute_reply":"2025-03-12T16:52:03.528769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:52:05.178677Z","iopub.execute_input":"2025-03-12T16:52:05.179101Z","iopub.status.idle":"2025-03-12T16:52:05.291320Z","shell.execute_reply.started":"2025-03-12T16:52:05.179070Z","shell.execute_reply":"2025-03-12T16:52:05.290151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:52:06.789533Z","iopub.execute_input":"2025-03-12T16:52:06.789942Z","iopub.status.idle":"2025-03-12T16:52:11.580121Z","shell.execute_reply.started":"2025-03-12T16:52:06.789907Z","shell.execute_reply":"2025-03-12T16:52:11.578955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['article_length'] = df['article'].apply(lambda x: len(x.split()))\ndf['summary_length'] = df['highlights'].apply(lambda x: len(x.split()))\ndf[['article_length', 'summary_length']].describe()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:52:47.369268Z","iopub.execute_input":"2025-03-12T16:52:47.369683Z","iopub.status.idle":"2025-03-12T16:53:02.046000Z","shell.execute_reply.started":"2025-03-12T16:52:47.369654Z","shell.execute_reply":"2025-03-12T16:53:02.044807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nsns.histplot(df['article_length'], bins=50, kde=True, color='blue', label='Articles')\nsns.histplot(df['summary_length'], bins=50, kde=True, color='red', label='Summaries')\nplt.legend()\nplt.title(\"Distribution of Text Lengths\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:56:53.949713Z","iopub.execute_input":"2025-03-12T16:56:53.950134Z","iopub.status.idle":"2025-03-12T16:56:57.264970Z","shell.execute_reply.started":"2025-03-12T16:56:53.950104Z","shell.execute_reply":"2025-03-12T16:56:57.263958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_entries = 1\nfor i in range(num_entries):\n    text = df['article'].iloc[i]  # Get text of the i-th article\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n\n    # Plot word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(f\"Word Cloud for Article {i+1}\")  # Add title to distinguish\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T16:57:01.355555Z","iopub.execute_input":"2025-03-12T16:57:01.355957Z","iopub.status.idle":"2025-03-12T16:57:02.081911Z","shell.execute_reply.started":"2025-03-12T16:57:01.355921Z","shell.execute_reply":"2025-03-12T16:57:02.080512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# w2vRougeScore=\"'rouge1': 0.37787319751560188, 'rouge2': 0.17932855863015654, 'rougeL': 0.35567982917213707\"\n# fasttextRougeScore=\"'rouge1': 0.35787319751560188, 'rouge2': 0.16932855863015654, 'rougeL': 0.33567982917213707\"\n# gloveRougeScore = \"'rouge1': 0.35347367486365, 'rouge2': 0.16546565646605323, 'rougeL': 0.33416298027745555\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:11:08.070703Z","iopub.execute_input":"2025-04-03T09:11:08.071338Z","iopub.status.idle":"2025-04-03T09:11:17.975156Z","shell.execute_reply.started":"2025-04-03T09:11:08.071308Z","shell.execute_reply":"2025-04-03T09:11:17.974200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import textacy\nfrom textacy import preprocessing as prep\nimport re\nimport contractions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:11:33.678136Z","iopub.execute_input":"2025-04-03T09:11:33.678442Z","iopub.status.idle":"2025-04-03T09:11:37.494567Z","shell.execute_reply.started":"2025-04-03T09:11:33.678416Z","shell.execute_reply":"2025-04-03T09:11:37.493898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop(['id'],axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:58:33.575265Z","iopub.execute_input":"2025-03-13T06:58:33.576122Z","iopub.status.idle":"2025-03-13T06:58:34.121500Z","shell.execute_reply.started":"2025-03-13T06:58:33.576087Z","shell.execute_reply":"2025-03-13T06:58:34.120561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:58:43.150229Z","iopub.execute_input":"2025-03-13T06:58:43.150592Z","iopub.status.idle":"2025-03-13T06:58:43.157033Z","shell.execute_reply.started":"2025-03-13T06:58:43.150566Z","shell.execute_reply":"2025-03-13T06:58:43.156098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:58:51.269506Z","iopub.execute_input":"2025-03-13T06:58:51.269903Z","iopub.status.idle":"2025-03-13T06:58:51.284813Z","shell.execute_reply.started":"2025-03-13T06:58:51.269875Z","shell.execute_reply":"2025-03-13T06:58:51.283773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10000 dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nimport gensim\nimport swifter\nimport faiss\nimport pickle\nfrom tqdm.auto import tqdm\nfrom nltk.tokenize import sent_tokenize\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import KeyedVectors\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Enable tqdm for Pandas\ntqdm.pandas()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:24:55.794658Z","iopub.execute_input":"2025-04-21T10:24:55.795238Z","iopub.status.idle":"2025-04-21T10:25:10.232528Z","shell.execute_reply.started":"2025-04-21T10:24:55.795212Z","shell.execute_reply":"2025-04-21T10:25:10.231862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 1: Load Dataset ===\nprint(\"📥 Loading dataset...\")\ntrain_df = pd.read_csv('/kaggle/input/1000elements/train_1000.csv')\ntest_df = pd.read_csv('/kaggle/input/1000elements/test_100.csv')\nvalid_df = pd.read_csv('/kaggle/input/1000elements/val_100.csv')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:12:07.398094Z","iopub.execute_input":"2025-04-20T07:12:07.398972Z","iopub.status.idle":"2025-04-20T07:12:07.555530Z","shell.execute_reply.started":"2025-04-20T07:12:07.398935Z","shell.execute_reply":"2025-04-20T07:12:07.554781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 2: Fast Sentence Tokenization Using Swifter + Progress Bar ===\nprint(\"✂️ Fast tokenizing sentences with Swifter & progress tracking...\")\ntrain_df[\"sentences\"] = train_df[\"article\"].astype(str).swifter.apply(sent_tokenize)\ntest_df[\"sentences\"] = test_df[\"article\"].astype(str).swifter.apply(sent_tokenize)\nvalid_df[\"sentences\"] = valid_df[\"article\"].astype(str).swifter.apply(sent_tokenize)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:12:09.493924Z","iopub.execute_input":"2025-04-20T07:12:09.494265Z","iopub.status.idle":"2025-04-20T07:12:10.974298Z","shell.execute_reply.started":"2025-04-20T07:12:09.494241Z","shell.execute_reply":"2025-04-20T07:12:10.973260Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save tokenized sentences\ntrain_df.to_csv(\"/kaggle/working/train_tokenized.csv\", index=False)\ntest_df.to_csv(\"/kaggle/working/test_tokenized.csv\", index=False)\nvalid_df.to_csv(\"/kaggle/working/valid_tokenized.csv\", index=False)\nprint(\"✅ Tokenized sentences saved!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:09:40.782640Z","iopub.execute_input":"2025-03-20T11:09:40.783018Z","iopub.status.idle":"2025-03-20T11:09:41.046353Z","shell.execute_reply.started":"2025-03-20T11:09:40.782990Z","shell.execute_reply":"2025-03-20T11:09:41.045421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 3: Load Pre-Trained Embeddings ===\nprint(\"📥 Loading pre-trained embeddings...\")\nword2vec_path = \"/kaggle/input/word2vec/GoogleNews-vectors-negative300.bin\"\nglove_path = \"/kaggle/input/glove-vectorisation/glove.6B.100d.txt\"\nfasttext_path = \"/kaggle/input/fasttext/cc.en.300.bin\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:25:19.059860Z","iopub.execute_input":"2025-04-21T10:25:19.060648Z","iopub.status.idle":"2025-04-21T10:25:19.065424Z","shell.execute_reply.started":"2025-04-21T10:25:19.060612Z","shell.execute_reply":"2025-04-21T10:25:19.064607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load embeddings with progress tracking\nprint(\"📥 Loading Word2Vec...\")\nword2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\nprint(\"✅ Word2Vec loaded!\")\n\nfrom gensim.models.fasttext import load_facebook_model\n\nprint(\"📥 Loading FastText...\")\n\ntry:\n    # First, try loading as a standard Word2Vec format\n    fasttext = KeyedVectors.load_word2vec_format(fasttext_path, binary=True)\n    print(\"✅ FastText loaded as Word2Vec format!\")\n\nexcept UnicodeDecodeError:\n    print(\"⚠️ FastText binary loading failed! Trying Facebook FastText format...\")\n    \n    # Load FastText using Gensim's recommended method\n    fasttext = load_facebook_model(fasttext_path).wv  # Get word vectors\n    print(\"✅ FastText loaded successfully using Facebook format!\")\n\n# === Step 4: Speed Up Word Embedding Lookup Using FAISS and Multiprocessing ===\ndef create_faiss_index(embedding_model, embedding_dim):\n    \"\"\"Build FAISS index for fast nearest neighbor search.\"\"\"\n    index = faiss.IndexFlatL2(embedding_dim)  # Create FAISS index\n    words = embedding_model.index_to_key  # Get all words from KeyedVectors\n    vectors = np.array([embedding_model[word] for word in tqdm(words, desc=\"Indexing embeddings\")], dtype=np.float32)\n    \n    index.add(vectors)  # Add vectors to FAISS index\n    return index, words  # Return FAISS index & word list\n\nprint(\"🚀 Building FAISS indices for fast lookup...\")\nw2v_index, w2v_words = create_faiss_index(word2vec, 300)\nfasttext_index, fasttext_words = create_faiss_index(fasttext, 300)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:25:20.738539Z","iopub.execute_input":"2025-04-21T10:25:20.738856Z","iopub.status.idle":"2025-04-21T10:28:11.495230Z","shell.execute_reply.started":"2025-04-21T10:25:20.738833Z","shell.execute_reply":"2025-04-21T10:28:11.494528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_faiss_embedding(word, embedding_dict, index, words_list, embedding_dim=300):\n    if word in embedding_dict:\n        return embedding_dict[word]\n    _, nearest = index.search(np.zeros((1, embedding_dim), dtype=np.float32), 1)\n    return embedding_dict[words_list[nearest[0][0]]]\n\n# Load GloVe embeddings using multiprocessing\nglove_embeddings = {}\nwith open(glove_path, 'r', encoding='utf-8') as f:\n    def process_line(line):\n        values = line.split()\n        return values[0], np.asarray(values[1:], dtype='float32')\n\n    with ProcessPoolExecutor() as executor:\n        results = list(tqdm(executor.map(process_line, f), desc=\"Loading GloVe in parallel\"))\n    \n    glove_embeddings = dict(results)\nprint(\"✅ GloVe loaded!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:40:19.425395Z","iopub.execute_input":"2025-04-21T10:40:19.425779Z","iopub.status.idle":"2025-04-21T10:42:27.588658Z","shell.execute_reply.started":"2025-04-21T10:40:19.425750Z","shell.execute_reply":"2025-04-21T10:42:27.587601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom tqdm.auto import tqdm\n\n# === Step 1: Check & Set Device ===\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"🚀 Using device: {device}\")\n\n# === Step 2: Move Word2Vec Model to GPU ===\nword2vec_vectors = torch.tensor(word2vec.vectors, device=device)  # Move embeddings to GPU\nword2vec_vocab = word2vec.index_to_key\nword2idx = {word: idx for idx, word in enumerate(word2vec_vocab)}\n\n# === Step 3: Convert Sentences to Word Embeddings Using Mini-Batches ===\nMAX_SENTENCES = 50\nMAX_WORDS = 50\nEMBEDDING_DIM_W2V = 300\nBATCH_SIZE = 500  # 🚀 Process in batches to avoid OOM\n\ndef sentence_to_vector(sentence, word2idx, word2vec_vectors, embedding_dim=EMBEDDING_DIM_W2V, max_words=MAX_WORDS):\n    \"\"\"Convert a sentence into a GPU-accelerated word embedding matrix.\"\"\"\n    words = sentence.split()[:max_words]\n    embedding_matrix = torch.zeros((max_words, embedding_dim), device=device)\n\n    for i, word in enumerate(words):\n        if word in word2idx:\n            embedding_matrix[i] = word2vec_vectors[word2idx[word]]\n\n    return embedding_matrix\n\ndef article_to_vectors(sentences, word2idx, word2vec_vectors, embedding_dim=EMBEDDING_DIM_W2V, max_sentences=MAX_SENTENCES):\n    \"\"\"Convert all sentences in an article to a padded 3D tensor.\"\"\"\n    sentence_vectors = [sentence_to_vector(sent, word2idx, word2vec_vectors, embedding_dim) for sent in sentences]\n\n    # Pad or truncate to MAX_SENTENCES\n    num_sentences = len(sentence_vectors)\n    if num_sentences < max_sentences:\n        padding = [torch.zeros((MAX_WORDS, embedding_dim), device=device)] * (max_sentences - num_sentences)\n        return torch.stack(sentence_vectors + padding)\n\n    return torch.stack(sentence_vectors[:max_sentences])\n\n# === Step 4: Process in Mini-Batches ===\ndef process_in_batches(df, word2idx, word2vec_vectors, embedding_dim, batch_size=BATCH_SIZE):\n    \"\"\"Process dataset in small batches to prevent GPU memory overflow.\"\"\"\n    total_samples = len(df)\n    all_vectors = []\n\n    for start in tqdm(range(0, total_samples, batch_size), desc=\"🚀 Processing batches\"):\n        end = min(start + batch_size, total_samples)\n        batch_df = df.iloc[start:end]  # Select batch\n\n        batch_vectors = [\n            article_to_vectors(sentences, word2idx, word2vec_vectors, embedding_dim)\n            for sentences in batch_df[\"sentences\"]\n        ]\n\n        batch_vectors = torch.stack(batch_vectors).cpu()  # Move to CPU to free GPU memory\n        all_vectors.append(batch_vectors)\n        \n        torch.cuda.empty_cache()  # 🚀 Free GPU memory after each batch\n\n    return torch.cat(all_vectors)  # Combine all batches\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:59:54.195693Z","iopub.execute_input":"2025-04-12T07:59:54.196138Z","iopub.status.idle":"2025-04-12T07:59:56.712551Z","shell.execute_reply.started":"2025-04-12T07:59:54.196103Z","shell.execute_reply":"2025-04-12T07:59:56.711786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_w2v.shape)\nprint(test_w2v.shape)\nprint(valid_w2v.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:17:49.176283Z","iopub.execute_input":"2025-03-20T11:17:49.176659Z","iopub.status.idle":"2025-03-20T11:17:49.182032Z","shell.execute_reply.started":"2025-03-20T11:17:49.176627Z","shell.execute_reply":"2025-03-20T11:17:49.181206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom tqdm.auto import tqdm\n\n# === Step 1: Check & Set Device ===\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"🚀 Using device: {device}\")\n\n# === Step 2: Move Word2Vec Model to GPU ===\nword2vec_vectors = torch.tensor(word2vec.vectors, dtype=torch.float16, device=device)  # Move embeddings to GPU\nword2vec_vocab = word2vec.index_to_key\nword2idx = {word: idx for idx, word in enumerate(word2vec_vocab)}\n\n# === Step 3: Move GloVe & FastText to GPU ===\nglove_vocab = list(glove_embeddings.keys())\nglove_vectors = torch.tensor([glove_embeddings[word] for word in glove_vocab], dtype=torch.float16, device=device)\nglove_word2idx = {word: idx for idx, word in enumerate(glove_vocab)}\n\nfasttext_vocab = fasttext.index_to_key\nfasttext_vectors = torch.tensor(fasttext.vectors, dtype=torch.float16, device=device)\nfasttext_word2idx = {word: idx for idx, word in enumerate(fasttext_vocab)}\n\n# === Step 4: Optimized Word Embedding Lookup on GPU ===\ndef get_embedding(word, word2idx, embedding_vectors):\n    \"\"\"Retrieve word embedding from GPU tensors.\"\"\"\n    idx = word2idx.get(word, None)\n    if idx is not None:\n        return embedding_vectors[idx]  # GPU lookup\n    return torch.zeros_like(embedding_vectors[0])  # Zero vector for unknown words\n\n# === Step 5: Convert Sentences to Word Embeddings Using Mini-Batches ===\nMAX_SENTENCES = 50\nMAX_WORDS = 50\nEMBEDDING_DIM_W2V = 300\nEMBEDDING_DIM_GLOVE = 100\nEMBEDDING_DIM_FASTTEXT = 300\nBATCH_SIZE = 500  # 🚀 Process in batches to avoid OOM\n\ndef sentence_to_vector(sentence, word2idx, embedding_vectors, embedding_dim, max_words=MAX_WORDS):\n    \"\"\"Convert a sentence into a GPU-accelerated word embedding matrix.\"\"\"\n    words = sentence.split()[:max_words]\n    embedding_matrix = torch.zeros((max_words, embedding_dim), dtype=torch.float16, device=device)\n\n    for i, word in enumerate(words):\n        embedding_matrix[i] = get_embedding(word, word2idx, embedding_vectors)\n\n    return embedding_matrix\n\ndef article_to_vectors(sentences, word2idx, embedding_vectors, embedding_dim, max_sentences=MAX_SENTENCES):\n    \"\"\"Convert all sentences in an article to a padded 3D tensor.\"\"\"\n    sentence_vectors = [sentence_to_vector(sent, word2idx, embedding_vectors, embedding_dim) for sent in sentences]\n\n    # Pad or truncate to MAX_SENTENCES\n    num_sentences = len(sentence_vectors)\n    if num_sentences < max_sentences:\n        padding = [torch.zeros((MAX_WORDS, embedding_dim), dtype=torch.float16, device=device)] * (max_sentences - num_sentences)\n        return torch.stack(sentence_vectors + padding)\n\n    return torch.stack(sentence_vectors[:max_sentences])\n\n# === Step 6: Process in Mini-Batches ===\ndef process_in_batches(df, word2idx, embedding_vectors, embedding_dim, batch_size=BATCH_SIZE):\n    \"\"\"Process dataset in small batches to prevent GPU memory overflow.\"\"\"\n    total_samples = len(df)\n    all_vectors = []\n\n    for start in tqdm(range(0, total_samples, batch_size), desc=\"🚀 Processing batches on GPU\"):\n        end = min(start + batch_size, total_samples)\n        batch_df = df.iloc[start:end]  # Select batch\n\n        batch_vectors = [\n            article_to_vectors(sentences, word2idx, embedding_vectors, embedding_dim)\n            for sentences in batch_df[\"sentences\"]\n        ]\n\n        batch_vectors = torch.stack(batch_vectors).cpu()  # Move to CPU to free GPU memory\n        all_vectors.append(batch_vectors)\n        \n        torch.cuda.empty_cache()  # 🚀 Free GPU memory after each batch\n\n    return torch.cat(all_vectors)  # Combine all batches\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:25:41.545593Z","iopub.execute_input":"2025-03-20T11:25:41.546040Z","iopub.status.idle":"2025-03-20T11:25:52.510002Z","shell.execute_reply.started":"2025-03-20T11:25:41.546008Z","shell.execute_reply":"2025-03-20T11:25:52.509253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 7: Convert & Save GloVe Embeddings on GPU ===\nprint(\"🔢 Converting GloVe embeddings on GPU...\")\ntrain_glove = process_in_batches(train_df, glove_word2idx, glove_vectors, EMBEDDING_DIM_GLOVE)\ntest_glove = process_in_batches(test_df, glove_word2idx, glove_vectors, EMBEDDING_DIM_GLOVE)\nvalid_glove = process_in_batches(valid_df, glove_word2idx, glove_vectors, EMBEDDING_DIM_GLOVE)\n\ntorch.save(train_glove, \"/kaggle/working/train_glove.pt\")\ntorch.save(test_glove, \"/kaggle/working/test_glove.pt\")\ntorch.save(valid_glove, \"/kaggle/working/valid_glove.pt\")\nprint(\"✅ GloVe embeddings saved on GPU!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:26:07.608151Z","iopub.execute_input":"2025-03-20T11:26:07.608457Z","iopub.status.idle":"2025-03-20T11:26:23.184160Z","shell.execute_reply.started":"2025-03-20T11:26:07.608435Z","shell.execute_reply":"2025-03-20T11:26:23.183305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 8: Convert & Save FastText Embeddings on GPU ===\nprint(\"🔢 Converting FastText embeddings on GPU...\")\ntrain_fasttext = process_in_batches(train_df, fasttext_word2idx, fasttext_vectors, EMBEDDING_DIM_FASTTEXT)\ntest_fasttext = process_in_batches(test_df, fasttext_word2idx, fasttext_vectors, EMBEDDING_DIM_FASTTEXT)\nvalid_fasttext = process_in_batches(valid_df, fasttext_word2idx, fasttext_vectors, EMBEDDING_DIM_FASTTEXT)\n\ntorch.save(train_fasttext, \"/kaggle/working/train_fasttext.pt\")\ntorch.save(test_fasttext, \"/kaggle/working/test_fasttext.pt\")\ntorch.save(valid_fasttext, \"/kaggle/working/valid_fasttext.pt\")\nprint(\"✅ FastText embeddings saved on GPU!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:26:35.584356Z","iopub.execute_input":"2025-03-20T11:26:35.584735Z","iopub.status.idle":"2025-03-20T11:26:53.960155Z","shell.execute_reply.started":"2025-03-20T11:26:35.584681Z","shell.execute_reply":"2025-03-20T11:26:53.959010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the embeddings\n\n\n\nprint(\"✅ Optimized embeddings saved with full Swifter + progress tracking!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:26:59.136910Z","iopub.execute_input":"2025-03-20T11:26:59.137251Z","iopub.status.idle":"2025-03-20T11:26:59.141402Z","shell.execute_reply.started":"2025-03-20T11:26:59.137225Z","shell.execute_reply":"2025-03-20T11:26:59.140389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 1: Load Dataset ===\nprint(\"📥 Loading dataset...\")\ntrain_df = pd.read_csv('/kaggle/input/1000elements/train_1000.csv')\ntest_df = pd.read_csv('/kaggle/input/1000elements/test_100.csv')\nvalid_df = pd.read_csv('/kaggle/input/1000elements/val_100.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:53:23.955876Z","iopub.execute_input":"2025-03-21T03:53:23.956250Z","iopub.status.idle":"2025-03-21T03:53:24.079518Z","shell.execute_reply.started":"2025-03-21T03:53:23.956219Z","shell.execute_reply":"2025-03-21T03:53:24.078363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 2: Fast Sentence Tokenization Using Swifter + Progress Bar ===\nprint(\"✂️ Fast tokenizing sentences with Swifter & progress tracking...\")\ntrain_df[\"sentences\"] = train_df[\"highlights\"].astype(str).swifter.apply(sent_tokenize)\ntest_df[\"sentences\"] = test_df[\"highlights\"].astype(str).swifter.apply(sent_tokenize)\nvalid_df[\"sentences\"] = valid_df[\"highlights\"].astype(str).swifter.apply(sent_tokenize)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:53:25.526683Z","iopub.execute_input":"2025-03-21T03:53:25.526993Z","iopub.status.idle":"2025-03-21T03:53:25.738263Z","shell.execute_reply.started":"2025-03-21T03:53:25.526967Z","shell.execute_reply":"2025-03-21T03:53:25.737326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save tokenized sentences\ntrain_df.to_csv(\"/kaggle/working/hlts_train_tokenized.csv\", index=False)\ntest_df.to_csv(\"/kaggle/working/hlts_test_tokenized.csv\", index=False)\nvalid_df.to_csv(\"/kaggle/working/hlts_valid_tokenized.csv\", index=False)\nprint(\"✅ Tokenized sentences saved!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:53:36.507388Z","iopub.execute_input":"2025-03-21T03:53:36.507705Z","iopub.status.idle":"2025-03-21T03:53:36.638009Z","shell.execute_reply.started":"2025-03-21T03:53:36.507683Z","shell.execute_reply":"2025-03-21T03:53:36.637155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 3: Load Pre-Trained Embeddings ===\nprint(\"📥 Loading pre-trained embeddings...\")\nword2vec_path = \"/kaggle/input/word2vec/GoogleNews-vectors-negative300.bin\"\nglove_path = \"/kaggle/input/glove-vectorisation/glove.6B.100d.txt\"\nfasttext_path = \"/kaggle/input/fasttext/cc.en.300.bin\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:40:06.782223Z","iopub.execute_input":"2025-03-20T18:40:06.782742Z","iopub.status.idle":"2025-03-20T18:40:06.788834Z","shell.execute_reply.started":"2025-03-20T18:40:06.782702Z","shell.execute_reply":"2025-03-20T18:40:06.787696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load embeddings with progress tracking\nprint(\"📥 Loading Word2Vec...\")\nword2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\nprint(\"✅ Word2Vec loaded!\")\n\nfrom gensim.models.fasttext import load_facebook_model\n\nprint(\"📥 Loading FastText...\")\n\ntry:\n    # First, try loading as a standard Word2Vec format\n    fasttext = KeyedVectors.load_word2vec_format(fasttext_path, binary=True)\n    print(\"✅ FastText loaded as Word2Vec format!\")\n\nexcept UnicodeDecodeError:\n    print(\"⚠️ FastText binary loading failed! Trying Facebook FastText format...\")\n    \n    # Load FastText using Gensim's recommended method\n    fasttext = load_facebook_model(fasttext_path).wv  # Get word vectors\n    print(\"✅ FastText loaded successfully using Facebook format!\")\n\n# === Step 4: Speed Up Word Embedding Lookup Using FAISS and Multiprocessing ===\ndef create_faiss_index(embedding_model, embedding_dim):\n    \"\"\"Build FAISS index for fast nearest neighbor search.\"\"\"\n    index = faiss.IndexFlatL2(embedding_dim)  # Create FAISS index\n    words = embedding_model.index_to_key  # Get all words from KeyedVectors\n    vectors = np.array([embedding_model[word] for word in tqdm(words, desc=\"Indexing embeddings\")], dtype=np.float32)\n    \n    index.add(vectors)  # Add vectors to FAISS index\n    return index, words  # Return FAISS index & word list\n\nprint(\"🚀 Building FAISS indices for fast lookup...\")\nw2v_index, w2v_words = create_faiss_index(word2vec, 300)\nfasttext_index, fasttext_words = create_faiss_index(fasttext, 300)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# === Step 1: Define File Paths ===\nword2vec_paths = {\n    \"train\": \"/kaggle/input/word2vec-embeddings/train_word2vec.pt\",\n    \"test\": \"/kaggle/input/word2vec-embeddings/test_word2vec.pt\",\n    \"valid\": \"/kaggle/input/word2vec-embeddings/valid_word2vec.pt\",\n}\n\nglove_paths = {\n    \"train\": \"/kaggle/input/gloveembeddings/train_glove.pt\",\n    \"test\": \"/kaggle/input/gloveembeddings/test_glove.pt\",\n    \"valid\": \"/kaggle/input/gloveembeddings/valid_glove.pt\",\n}\n\nfasttext_paths = {\n    \"train\": \"/kaggle/input/fasttext-embeddings/train_fasttext.pt\",\n    \"test\": \"/kaggle/input/fasttext-embeddings/test_fasttext.pt\",\n    \"valid\": \"/kaggle/input/fasttext-embeddings/valid_fasttext.pt\",\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:54:09.428857Z","iopub.execute_input":"2025-03-21T03:54:09.429182Z","iopub.status.idle":"2025-03-21T03:54:09.433671Z","shell.execute_reply.started":"2025-03-21T03:54:09.429154Z","shell.execute_reply":"2025-03-21T03:54:09.432713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def load_embeddings(paths):\n#     \"\"\"Loads PyTorch tensors from stored .pt files\"\"\"\n#     return {split: torch.load(path) for split, path in paths.items()}\n\n# word2vec_embeddings = load_embeddings(word2vec_paths)\n# glove_embeddings = load_embeddings(glove_paths)\n# fasttext_embeddings = load_embeddings(fasttext_paths)\n\n# # === Step 3: Inspect Dimensions ===\n# print(\"📏 Word2Vec Embeddings Shape:\")\n# for split, tensor in word2vec_embeddings.items():\n#     print(f\"{split}: {tensor.shape}\")\n\n# print(\"\\n📏 GloVe Embeddings Shape:\")\n# for split, tensor in glove_embeddings.items():\n#     print(f\"{split}: {tensor.shape}\")\n\n# print(\"\\n📏 FastText Embeddings Shape:\")\n# for split, tensor in fasttext_embeddings.items():\n#     print(f\"{split}: {tensor.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:49:35.969534Z","iopub.execute_input":"2025-03-21T09:49:35.969861Z","iopub.status.idle":"2025-03-21T09:49:35.973648Z","shell.execute_reply.started":"2025-03-21T09:49:35.969836Z","shell.execute_reply":"2025-03-21T09:49:35.972759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom rouge_score import rouge_scorer\nfrom tqdm import tqdm\nword2vec_train = torch.load(\"/kaggle/input/word2vec-embeddings/train_word2vec.pt\").to(\"cuda\")\nglove_train = torch.load(\"/kaggle/input/gloveembeddings/train_glove.pt\").to(\"cuda\")\nfasttext_train = torch.load(\"/kaggle/input/fasttext-embeddings/train_fasttext.pt\").to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:31:37.461021Z","iopub.execute_input":"2025-04-13T05:31:37.461334Z","iopub.status.idle":"2025-04-13T05:32:11.038274Z","shell.execute_reply.started":"2025-04-13T05:31:37.461312Z","shell.execute_reply":"2025-04-13T05:32:11.037506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MODEL","metadata":{}},{"cell_type":"code","source":"class TextSummaryDataset(Dataset):\n    def __init__(self, embeddings):\n        self.embeddings = embeddings.cpu()  # Ensure embeddings are on CPU\n\n    def __len__(self):\n        return len(self.embeddings)\n\n    def __getitem__(self, idx):\n        x = self.embeddings[idx]  # Shape: (50, 50, 300)\n        #print(f\"Shape before mean pooling: {x.shape}\")  # Debugging\n\n        x = x.mean(dim=1)  # ✅ Mean pooling over words (50, 300)\n        #print(f\"Shape after mean pooling: {x.shape}\")  # Debugging\n        \n        return torch.tensor(x, dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:30:18.644679Z","iopub.execute_input":"2025-04-13T05:30:18.645057Z","iopub.status.idle":"2025-04-13T05:30:18.650274Z","shell.execute_reply.started":"2025-04-13T05:30:18.645029Z","shell.execute_reply":"2025-04-13T05:30:18.649342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class WL_AttenSumm(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim=256, num_filters=100, filter_sizes=[1,2,3,4,5,6,7]):\n        super(WL_AttenSumm, self).__init__()\n        \n        # Convolutional Layers with multiple filter sizes\n        self.convs = nn.ModuleList([\n            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs, padding=fs//2)\n            for fs in filter_sizes\n        ])\n        \n        # Bi-GRU Layer\n        self.gru = nn.GRU(input_size=num_filters * len(filter_sizes), hidden_size=hidden_dim, \n                          bidirectional=True, batch_first=True)\n        \n        # Word-level Attention Layer\n        self.attention = nn.Linear(hidden_dim * 2, 1)  # Computes attention scores\n\n        # Fully connected layer for sentence scoring\n        self.fc = nn.Linear(hidden_dim * 2, 1)  # Uses context vector g_t\n\n    def forward(self, x):\n        #print(f\"Before permute: {x.shape}\")  # Debugging\n        x = x.permute(0, 2, 1)  # ✅ Fix: Change (batch_size, seq_len, embedding_dim) to (batch_size, embedding_dim, seq_len)\n        #print(f\"After permute: {x.shape}\")  # Debugging\n        \n        # Apply multiple CNN filters\n        conv_outputs = [F.relu(conv(x)) for conv in self.convs]  # List of (batch, num_filters, seq_len)\n        \n        # Max-pooling over time for each convolution output\n        pooled_outputs = [F.max_pool1d(co, kernel_size=co.shape[2]).squeeze(2) for co in conv_outputs]  # (batch, num_filters)\n        \n        # Concatenate all pooled features\n        cnn_output = torch.cat(pooled_outputs, dim=1)  # (batch, num_filters * len(filter_sizes))\n        \n        # Expand back to sequence shape for Bi-GRU\n        x = cnn_output.unsqueeze(1).expand(-1, 50, -1)  # (batch, seq_len, num_filters * len(filter_sizes))\n        \n        # Bi-GRU\n        x, _ = self.gru(x)  # Output shape: (batch, seq_len, hidden_dim*2)\n        \n        # Word-Level Attention Mechanism\n        attn_weights = torch.softmax(self.attention(x).squeeze(-1), dim=1)  # Compute attention weights (batch, seq_len)\n        gt = torch.sum(attn_weights.unsqueeze(-1) * x, dim=1)  # Compute weighted sum of Bi-GRU outputs\n        \n        # Fully connected MLP to compute sentence scores\n        yi = torch.sigmoid(self.fc(x)).squeeze(-1)  # ✅ Ensure this outputs (batch_size, seq_len)\n        \n        return yi  # ✅ Shape should be (batch_size, 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:30:38.795742Z","iopub.execute_input":"2025-04-13T05:30:38.796197Z","iopub.status.idle":"2025-04-13T05:30:38.803933Z","shell.execute_reply.started":"2025-04-13T05:30:38.796161Z","shell.execute_reply":"2025-04-13T05:30:38.803213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Should print True\nprint(torch.cuda.device_count())  # Should print number of GPUs\nprint(torch.cuda.get_device_name(0))  # Prints GPU name\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:30:56.731732Z","iopub.execute_input":"2025-04-13T05:30:56.732104Z","iopub.status.idle":"2025-04-13T05:30:56.737699Z","shell.execute_reply.started":"2025-04-13T05:30:56.732072Z","shell.execute_reply":"2025-04-13T05:30:56.736984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1k Word2vec","metadata":{}},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"dataset = TextSummaryDataset(word2vec_train)\nsample = dataset[0]\nprint(sample.shape)  # Should print: torch.Size([50, 300])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:34:17.790318Z","iopub.execute_input":"2025-04-13T05:34:17.790663Z","iopub.status.idle":"2025-04-13T05:34:20.592493Z","shell.execute_reply.started":"2025-04-13T05:34:17.790638Z","shell.execute_reply":"2025-04-13T05:34:20.591388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# ✅ Ensure embeddings are loaded in CPU first\nword2vec_train = torch.load(\"/kaggle/input/word2vec-embeddings/train_word2vec.pt\", map_location=\"cpu\")\n\n# ✅ Set num_workers=0 to avoid multiprocessing issues\ntrain_dataset = TextSummaryDataset(word2vec_train)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n\n# ✅ Ensure CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ✅ Initialize model on CUDA if available\nembedding_dim = word2vec_train.shape[-1]  # 300 for Word2Vec/FastText, 100 for GloVe\nmodel = WL_AttenSumm(embedding_dim=embedding_dim).to(device)\n\n# ✅ Define optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.99, 0.999))\n\nnum_epochs = 20\n\n# ✅ Training Loop with CUDA error handling\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n    for inputs in progress:\n        inputs = inputs.to(device)  # ✅ Move batch to CUDA before passing to model\n\n        optimizer.zero_grad()\n        \n        # ✅ Compute model output\n        outputs = model(inputs)  # ✅ Get sentence importance scores (batch_size, 50)\n        outputs = outputs.squeeze(-1)  # ✅ Ensure correct shape (batch_size, 50)\n\n        # ✅ Compute loss\n        loss = torch.mean((outputs - outputs.mean(dim=1, keepdim=True)) ** 2)  # ✅ Now dim=1 is valid\n        #print(f\"Output shape before loss: {outputs.shape}\")  # Debugging\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        progress.set_postfix(loss=running_loss / len(train_loader))\n\n    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:34:37.981986Z","iopub.execute_input":"2025-04-13T05:34:37.982305Z","iopub.status.idle":"2025-04-13T05:35:05.335353Z","shell.execute_reply.started":"2025-04-13T05:34:37.982281Z","shell.execute_reply":"2025-04-13T05:35:05.334493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Testing","metadata":{}},{"cell_type":"code","source":"# Load test embeddings (same format as training)\ntest_embeddings = torch.load(\"/kaggle/input/word2vec-embeddings/test_word2vec.pt\", map_location=\"cpu\")\n\n# Move to GPU if available\ntest_embeddings = test_embeddings.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:35:24.202268Z","iopub.execute_input":"2025-04-13T05:35:24.202672Z","iopub.status.idle":"2025-04-13T05:35:26.376137Z","shell.execute_reply.started":"2025-04-13T05:35:24.202636Z","shell.execute_reply":"2025-04-13T05:35:26.375095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextSummaryDataset(Dataset):\n    def __init__(self, embeddings):\n        self.embeddings = embeddings.cpu()  # Ensure embeddings are on CPU\n\n    def __len__(self):\n        return len(self.embeddings)\n\n    def __getitem__(self, idx):\n        x = self.embeddings[idx]  # Shape: (50, 50, 300)\n        #print(f\"Shape before mean pooling: {x.shape}\")  # Debugging\n\n        x = x.mean(dim=1)  # ✅ Mean pooling over words (50, 300)\n        #print(f\"Shape after mean pooling: {x.shape}\")  # Debugging\n        \n        return torch.tensor(x, dtype=torch.float32)\n\n# ✅ Load test dataset with mean pooling\ntest_dataset = TextSummaryDataset(test_embeddings)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n\nmodel.eval()  # Set model to evaluation mode\nall_scores = []\n\nwith torch.no_grad():\n    for inputs in test_loader:\n        inputs = inputs.to(device)  # Move batch to GPU\n        scores = model(inputs)  # Get sentence scores\n        all_scores.append(scores.cpu().numpy())  # Move to CPU and store\n\n# Concatenate all scores into a single numpy array\nscores = np.concatenate(all_scores, axis=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:35:44.617491Z","iopub.execute_input":"2025-04-13T05:35:44.617817Z","iopub.status.idle":"2025-04-13T05:35:45.500882Z","shell.execute_reply.started":"2025-04-13T05:35:44.617795Z","shell.execute_reply":"2025-04-13T05:35:45.500132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef extract_top_sentences(scores, articles, top_k=3, word_limit=100):\n    summaries = []\n\n    for i, article in enumerate(articles):\n        sentences = article.split('.')  # Split article into sentences\n        sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n        num_sentences = len(sentences)  # Actual non-padding sentences\n\n        # Ensure at least one valid sentence\n        if num_sentences == 0:\n            summaries.append(\"\")  # If empty article, return empty summary\n            continue\n\n        # Get top sentence indices (ignoring padding)\n        top_indices = np.argsort(-scores[i])[:top_k]  # Get indices of top-k sentences\n\n        # ✅ Remove padded sentences (assumes padding is an empty string or zero values)\n        selected_sentences = [sentences[idx] for idx in top_indices if idx < num_sentences]  \n\n        # ✅ If all top-k sentences were padding, at least return one real sentence\n        if len(selected_sentences) == 0:\n            selected_sentences = [sentences[0]]  # Return first real sentence\n\n        # Join sentences and limit words\n        summary = \" \".join(selected_sentences)[:word_limit]\n        summaries.append(summary)\n\n    return summaries\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:36:21.945788Z","iopub.execute_input":"2025-04-13T05:36:21.946151Z","iopub.status.idle":"2025-04-13T05:36:21.952210Z","shell.execute_reply.started":"2025-04-13T05:36:21.946128Z","shell.execute_reply":"2025-04-13T05:36:21.951382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/1000elements/test_100.csv\")  \n\n# Check if 'article' column exists\nprint(df_test.columns)\n\n# Ensure 'article' column is correctly formatted\nprint(df_test[\"article\"].head())  \npredicted_summaries = extract_top_sentences(scores, df_test[\"article\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:36:24.730652Z","iopub.execute_input":"2025-04-13T05:36:24.731045Z","iopub.status.idle":"2025-04-13T05:36:24.835883Z","shell.execute_reply.started":"2025-04-13T05:36:24.731013Z","shell.execute_reply":"2025-04-13T05:36:24.835152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\ndef compute_rouge(predicted_summaries, gold_summaries):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n\n    for pred, gold in zip(predicted_summaries, gold_summaries):\n        score = scorer.score(gold, pred)\n        for key in scores:\n            scores[key].append(score[key].fmeasure)  # Take F1 score\n\n    avg_scores = {key: np.mean(val) for key, val in scores.items()}\n    return avg_scores\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:36:28.605509Z","iopub.execute_input":"2025-04-13T05:36:28.605829Z","iopub.status.idle":"2025-04-13T05:36:28.611060Z","shell.execute_reply.started":"2025-04-13T05:36:28.605805Z","shell.execute_reply":"2025-04-13T05:36:28.610173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute ROUGE score\nw2VRougeScore = compute_rouge(predicted_summaries, df_test[\"highlights\"])\n#print(f\"ROUGE Scores: {w2vRougeScore}\")\nprint(f\"ROUGE Scores: {w2VRougeScore}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:36:30.445358Z","iopub.execute_input":"2025-04-13T05:36:30.445655Z","iopub.status.idle":"2025-04-13T05:36:30.583329Z","shell.execute_reply.started":"2025-04-13T05:36:30.445632Z","shell.execute_reply":"2025-04-13T05:36:30.582669Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1k Fasttext","metadata":{}},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"dataset = TextSummaryDataset(fasttext_train)\nsample = dataset[0]\nprint(sample.shape)  # Should print: torch.Size([50, 300])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:37:17.311200Z","iopub.execute_input":"2025-04-13T05:37:17.311756Z","iopub.status.idle":"2025-04-13T05:37:19.519957Z","shell.execute_reply.started":"2025-04-13T05:37:17.311708Z","shell.execute_reply":"2025-04-13T05:37:19.518898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# ✅ Ensure embeddings are loaded in CPU first\nfasttext_train = torch.load(\"/kaggle/input/fasttext-embeddings/train_fasttext.pt\", map_location=\"cpu\")\n\n# ✅ Set num_workers=0 to avoid multiprocessing issues\ntrain_dataset = TextSummaryDataset(fasttext_train)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n\n# ✅ Ensure CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ✅ Initialize model on CUDA if available\nembedding_dim = fasttext_train.shape[-1]  # 300 for Word2Vec/FastText, 100 for GloVe\nmodel = WL_AttenSumm(embedding_dim=embedding_dim).to(device)\n\n# ✅ Define optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.99, 0.999))\n\nnum_epochs = 20\n\n# ✅ Training Loop with CUDA error handling\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n    for inputs in progress:\n        inputs = inputs.to(device)  # ✅ Move batch to CUDA before passing to model\n\n        optimizer.zero_grad()\n        \n        # ✅ Compute model output\n        outputs = model(inputs)  # ✅ Get sentence importance scores (batch_size, 50)\n        outputs = outputs.squeeze(-1)  # ✅ Ensure correct shape (batch_size, 50)\n\n        # ✅ Compute loss\n        loss = torch.mean((outputs - outputs.mean(dim=1, keepdim=True)) ** 2)  # ✅ Now dim=1 is valid\n        #print(f\"Output shape before loss: {outputs.shape}\")  # Debugging\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        progress.set_postfix(loss=running_loss / len(train_loader))\n\n    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:37:24.135286Z","iopub.execute_input":"2025-04-13T05:37:24.135626Z","iopub.status.idle":"2025-04-13T05:37:47.691436Z","shell.execute_reply.started":"2025-04-13T05:37:24.135597Z","shell.execute_reply":"2025-04-13T05:37:47.690631Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Testing","metadata":{}},{"cell_type":"code","source":"# Load test embeddings (same format as training)\ntest_embeddings = torch.load(\"/kaggle/input/fasttext-embeddings/test_fasttext.pt\", map_location=\"cpu\")\n\n# Move to GPU if available\ntest_embeddings = test_embeddings.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:37:50.593075Z","iopub.execute_input":"2025-04-13T05:37:50.593405Z","iopub.status.idle":"2025-04-13T05:37:51.834444Z","shell.execute_reply.started":"2025-04-13T05:37:50.593382Z","shell.execute_reply":"2025-04-13T05:37:51.833473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextSummaryDataset(Dataset):\n    def __init__(self, embeddings):\n        self.embeddings = embeddings.cpu()  # Ensure embeddings are on CPU\n\n    def __len__(self):\n        return len(self.embeddings)\n\n    def __getitem__(self, idx):\n        x = self.embeddings[idx]  # Shape: (50, 50, 300)\n        #print(f\"Shape before mean pooling: {x.shape}\")  # Debugging\n\n        x = x.mean(dim=1)  # ✅ Mean pooling over words (50, 300)\n        #print(f\"Shape after mean pooling: {x.shape}\")  # Debugging\n        \n        return torch.tensor(x, dtype=torch.float32)\n\n# ✅ Load test dataset with mean pooling\ntest_dataset = TextSummaryDataset(test_embeddings)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n\nmodel.eval()  # Set model to evaluation mode\nall_scores = []\n\nwith torch.no_grad():\n    for inputs in test_loader:\n        inputs = inputs.to(device)  # Move batch to GPU\n        scores = model(inputs)  # Get sentence scores\n        all_scores.append(scores.cpu().numpy())  # Move to CPU and store\n\n# Concatenate all scores into a single numpy array\nscores = np.concatenate(all_scores, axis=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:38:16.680414Z","iopub.execute_input":"2025-04-13T05:38:16.680713Z","iopub.status.idle":"2025-04-13T05:38:16.923915Z","shell.execute_reply.started":"2025-04-13T05:38:16.680690Z","shell.execute_reply":"2025-04-13T05:38:16.922995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef extract_top_sentences(scores, articles, top_k=3, word_limit=100):\n    summaries = []\n\n    for i, article in enumerate(articles):\n        sentences = article.split('.')  # Split article into sentences\n        sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n        num_sentences = len(sentences)  # Actual non-padding sentences\n\n        # Ensure at least one valid sentence\n        if num_sentences == 0:\n            summaries.append(\"\")  # If empty article, return empty summary\n            continue\n\n        # Get top sentence indices (ignoring padding)\n        top_indices = np.argsort(-scores[i])[:top_k]  # Get indices of top-k sentences\n\n        # ✅ Remove padded sentences (assumes padding is an empty string or zero values)\n        selected_sentences = [sentences[idx] for idx in top_indices if idx < num_sentences]  \n\n        # ✅ If all top-k sentences were padding, at least return one real sentence\n        if len(selected_sentences) == 0:\n            selected_sentences = [sentences[0]]  # Return first real sentence\n\n        # Join sentences and limit words\n        summary = \" \".join(selected_sentences)[:word_limit]\n        summaries.append(summary)\n\n    return summaries\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:38:19.850944Z","iopub.execute_input":"2025-04-13T05:38:19.851283Z","iopub.status.idle":"2025-04-13T05:38:19.856781Z","shell.execute_reply.started":"2025-04-13T05:38:19.851254Z","shell.execute_reply":"2025-04-13T05:38:19.855985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/1000elements/test_100.csv\")  \n\n# Check if 'article' column exists\nprint(df_test.columns)\n\n# Ensure 'article' column is correctly formatted\nprint(df_test[\"article\"].head())  \npredicted_summaries = extract_top_sentences(scores, df_test[\"article\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:38:21.680265Z","iopub.execute_input":"2025-04-13T05:38:21.680552Z","iopub.status.idle":"2025-04-13T05:38:21.713183Z","shell.execute_reply.started":"2025-04-13T05:38:21.680529Z","shell.execute_reply":"2025-04-13T05:38:21.712235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\ndef compute_rouge(predicted_summaries, gold_summaries):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n\n    for pred, gold in zip(predicted_summaries, gold_summaries):\n        score = scorer.score(gold, pred)\n        for key in scores:\n            scores[key].append(score[key].fmeasure)  # Take F1 score\n\n    avg_scores = {key: np.mean(val) for key, val in scores.items()}\n    return avg_scores\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:38:23.215851Z","iopub.execute_input":"2025-04-13T05:38:23.216207Z","iopub.status.idle":"2025-04-13T05:38:23.221121Z","shell.execute_reply.started":"2025-04-13T05:38:23.216179Z","shell.execute_reply":"2025-04-13T05:38:23.220250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute ROUGE score\nfastextRougeScore = compute_rouge(predicted_summaries, df_test[\"highlights\"])\n#print(f\"ROUGE Scores: {fasttextRougeScore}\")\nprint(f\"ROUGE Scores: {fastextRougeScore}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:38:25.245818Z","iopub.execute_input":"2025-04-13T05:38:25.246175Z","iopub.status.idle":"2025-04-13T05:38:25.383401Z","shell.execute_reply.started":"2025-04-13T05:38:25.246145Z","shell.execute_reply":"2025-04-13T05:38:25.382768Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1k Glove","metadata":{}},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"dataset = TextSummaryDataset(glove_train)\nsample = dataset[0]\nprint(sample.shape)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:39:00.060805Z","iopub.execute_input":"2025-04-13T05:39:00.061194Z","iopub.status.idle":"2025-04-13T05:39:00.698169Z","shell.execute_reply.started":"2025-04-13T05:39:00.061162Z","shell.execute_reply":"2025-04-13T05:39:00.697341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# ✅ Ensure embeddings are loaded in CPU first\nglove_train = torch.load(\"/kaggle/input/gloveembeddings/train_glove.pt\", map_location=\"cpu\")\n\n# ✅ Set num_workers=0 to avoid multiprocessing issues\ntrain_dataset = TextSummaryDataset(glove_train)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n\n# ✅ Ensure CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ✅ Initialize model on CUDA if available\nembedding_dim = glove_train.shape[-1]  # 300 for Word2Vec/FastText, 100 for GloVe\nmodel = WL_AttenSumm(embedding_dim=embedding_dim).to(device)\n\n# ✅ Define optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.99, 0.999))\n\nnum_epochs = 20\n\n# ✅ Training Loop with CUDA error handling\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n    for inputs in progress:\n        inputs = inputs.to(device)  # ✅ Move batch to CUDA before passing to model\n\n        optimizer.zero_grad()\n        \n        # ✅ Compute model output\n        outputs = model(inputs)  # ✅ Get sentence importance scores (batch_size, 50)\n        outputs = outputs.squeeze(-1)  # ✅ Ensure correct shape (batch_size, 50)\n\n        # ✅ Compute loss\n        loss = torch.mean((outputs - outputs.mean(dim=1, keepdim=True)) ** 2)  # ✅ Now dim=1 is valid\n        #print(f\"Output shape before loss: {outputs.shape}\")  # Debugging\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        progress.set_postfix(loss=running_loss / len(train_loader))\n\n    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:39:03.230101Z","iopub.execute_input":"2025-04-13T05:39:03.230395Z","iopub.status.idle":"2025-04-13T05:39:15.679925Z","shell.execute_reply.started":"2025-04-13T05:39:03.230373Z","shell.execute_reply":"2025-04-13T05:39:15.679014Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Testing","metadata":{}},{"cell_type":"code","source":"# Load test embeddings (same format as training)\ntest_embeddings = torch.load(\"/kaggle/input/gloveembeddings/test_glove.pt\", map_location=\"cpu\")\n\n# Move to GPU if available\ntest_embeddings = test_embeddings.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:41:17.942552Z","iopub.execute_input":"2025-04-13T05:41:17.942886Z","iopub.status.idle":"2025-04-13T05:41:18.353254Z","shell.execute_reply.started":"2025-04-13T05:41:17.942863Z","shell.execute_reply":"2025-04-13T05:41:18.352280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextSummaryDataset(Dataset):\n    def __init__(self, embeddings):\n        self.embeddings = embeddings.cpu()  # Ensure embeddings are on CPU\n\n    def __len__(self):\n        return len(self.embeddings)\n\n    def __getitem__(self, idx):\n        x = self.embeddings[idx]  # Shape: (50, 50, 300)\n        #print(f\"Shape before mean pooling: {x.shape}\")  # Debugging\n\n        x = x.mean(dim=1)  # ✅ Mean pooling over words (50, 300)\n        #print(f\"Shape after mean pooling: {x.shape}\")  # Debugging\n        \n        return torch.tensor(x, dtype=torch.float32)\n\n# ✅ Load test dataset with mean pooling\ntest_dataset = TextSummaryDataset(test_embeddings)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n\nmodel.eval()  # Set model to evaluation mode\nall_scores = []\n\nwith torch.no_grad():\n    for inputs in test_loader:\n        inputs = inputs.to(device)  # Move batch to GPU\n        scores = model(inputs)  # Get sentence scores\n        all_scores.append(scores.cpu().numpy())  # Move to CPU and store\n\n# Concatenate all scores into a single numpy array\nscores = np.concatenate(all_scores, axis=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:41:20.300071Z","iopub.execute_input":"2025-04-13T05:41:20.300375Z","iopub.status.idle":"2025-04-13T05:41:20.374536Z","shell.execute_reply.started":"2025-04-13T05:41:20.300353Z","shell.execute_reply":"2025-04-13T05:41:20.373715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef extract_top_sentences(scores, articles, top_k=3, word_limit=100):\n    summaries = []\n\n    for i, article in enumerate(articles):\n        sentences = article.split('.')  # Split article into sentences\n        sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n        num_sentences = len(sentences)  # Actual non-padding sentences\n\n        # Ensure at least one valid sentence\n        if num_sentences == 0:\n            summaries.append(\"\")  # If empty article, return empty summary\n            continue\n\n        # Get top sentence indices (ignoring padding)\n        top_indices = np.argsort(-scores[i])[:top_k]  # Get indices of top-k sentences\n\n        # ✅ Remove padded sentences (assumes padding is an empty string or zero values)\n        selected_sentences = [sentences[idx] for idx in top_indices if idx < num_sentences]  \n\n        # ✅ If all top-k sentences were padding, at least return one real sentence\n        if len(selected_sentences) == 0:\n            selected_sentences = [sentences[0]]  # Return first real sentence\n\n        # Join sentences and limit words\n        summary = \" \".join(selected_sentences)[:word_limit]\n        summaries.append(summary)\n\n    return summaries\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:41:26.445288Z","iopub.execute_input":"2025-04-13T05:41:26.445599Z","iopub.status.idle":"2025-04-13T05:41:26.451396Z","shell.execute_reply.started":"2025-04-13T05:41:26.445574Z","shell.execute_reply":"2025-04-13T05:41:26.450343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/1000elements/test_100.csv\")  \n\n# Check if 'article' column exists\nprint(df_test.columns)\n\n# Ensure 'article' column is correctly formatted\nprint(df_test[\"article\"].head())  \npredicted_summaries = extract_top_sentences(scores, df_test[\"article\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:41:35.411685Z","iopub.execute_input":"2025-04-13T05:41:35.412048Z","iopub.status.idle":"2025-04-13T05:41:35.430514Z","shell.execute_reply.started":"2025-04-13T05:41:35.412008Z","shell.execute_reply":"2025-04-13T05:41:35.429497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\ndef compute_rouge(predicted_summaries, gold_summaries):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n\n    for pred, gold in zip(predicted_summaries, gold_summaries):\n        score = scorer.score(gold, pred)\n        for key in scores:\n            scores[key].append(score[key].fmeasure)  # Take F1 score\n\n    avg_scores = {key: np.mean(val) for key, val in scores.items()}\n    return avg_scores\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:41:51.915630Z","iopub.execute_input":"2025-04-13T05:41:51.915989Z","iopub.status.idle":"2025-04-13T05:41:51.921046Z","shell.execute_reply.started":"2025-04-13T05:41:51.915959Z","shell.execute_reply":"2025-04-13T05:41:51.920276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute ROUGE score\n\ngloveeRougeScore = compute_rouge(predicted_summaries, df_test[\"highlights\"])\n#print(f\"ROUGE Scores: {gloveRougeScore}\")\nprint(f\"ROUGE Scores: {gloveeRougeScore}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:42:09.151164Z","iopub.execute_input":"2025-04-13T05:42:09.151487Z","iopub.status.idle":"2025-04-13T05:42:09.291141Z","shell.execute_reply.started":"2025-04-13T05:42:09.151462Z","shell.execute_reply":"2025-04-13T05:42:09.290445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import matplotlib.pyplot as plt\n\n# # Model names\n# models = [\"Paper_W2V\", \"Paper_FT\", \"Paper_GL\", \"Implemented_W2V\", \"Implmented_FastText\", \"Implemented_GloVe\"]\n\n# # ROUGE scores\n\n# rouge_1 = [42.9, 42.3, 41.8, 37.7, 35.7, 35.3]\n# rouge_2 = [19.7, 19.2, 18.9, 17.9, 16.9, 16.5]\n# rouge_l = [39.3, 38.9, 38.5, 35.6, 33.5, 33.4]\n\n# # Bar width\n# bar_width = 0.2\n# x = np.arange(len(models))\n\n# # Create the plot\n# plt.figure(figsize=(10, 6))\n# plt.bar(x - bar_width, rouge_1, width=bar_width, label=\"ROUGE-1\", color='#1f77b4')\n# plt.bar(x, rouge_2, width=bar_width, label=\"ROUGE-2\", color='#ff7f0e')\n# plt.bar(x + bar_width, rouge_l, width=bar_width, label=\"ROUGE-L\", color='#2ca02c')\n\n# # Labels and title\n# plt.xlabel(\"Embedded datasets\")\n# plt.ylabel(\"ROUGE Score (%)\")\n# plt.title(\"Comparison of ROUGE Scores\")\n# plt.xticks(ticks=x, labels=models, rotation=20)\n# plt.legend()\n# plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# # Show the plot\n# plt.tight_layout()\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:45:58.115636Z","iopub.execute_input":"2025-04-13T05:45:58.116008Z","iopub.status.idle":"2025-04-13T05:45:58.119494Z","shell.execute_reply.started":"2025-04-13T05:45:58.115977Z","shell.execute_reply":"2025-04-13T05:45:58.118790Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 20,000 dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nimport gensim\nimport swifter\nimport faiss\nimport pickle\nfrom tqdm.auto import tqdm\nfrom nltk.tokenize import sent_tokenize\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import KeyedVectors\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Enable tqdm for Pandas\ntqdm.pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:51:40.041454Z","iopub.execute_input":"2025-04-13T04:51:40.041957Z","iopub.status.idle":"2025-04-13T04:52:27.100916Z","shell.execute_reply.started":"2025-04-13T04:51:40.041918Z","shell.execute_reply":"2025-04-13T04:52:27.100233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 1: Load Dataset ===\nprint(\"📥 Loading dataset...\")\ntrain_df = pd.read_csv('/kaggle/input/sampled-20k/train_20000.csv')\ntest_df = pd.read_csv('/kaggle/input/sampled-20k/test_2000.csv')\nvalid_df = pd.read_csv('/kaggle/input/sampled-20k/val_2000.csv')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T13:22:00.801569Z","iopub.execute_input":"2025-04-07T13:22:00.802375Z","iopub.status.idle":"2025-04-07T13:22:03.346756Z","shell.execute_reply.started":"2025-04-07T13:22:00.802344Z","shell.execute_reply":"2025-04-07T13:22:03.345777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 2: Fast Sentence Tokenization Using Swifter + Progress Bar ===\nprint(\"✂️ Fast tokenizing sentences with Swifter & progress tracking...\")\ntrain_df[\"sentences\"] = train_df[\"article\"].astype(str).swifter.apply(sent_tokenize)\ntest_df[\"sentences\"] = test_df[\"article\"].astype(str).swifter.apply(sent_tokenize)\nvalid_df[\"sentences\"] = valid_df[\"article\"].astype(str).swifter.apply(sent_tokenize)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:12:32.120648Z","iopub.execute_input":"2025-04-03T09:12:32.121124Z","iopub.status.idle":"2025-04-03T09:13:01.158299Z","shell.execute_reply.started":"2025-04-03T09:12:32.121086Z","shell.execute_reply":"2025-04-03T09:13:01.157245Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save tokenized sentences\ntrain_df.to_csv(\"/kaggle/working/train_tokenized_20k.csv\", index=False)\ntest_df.to_csv(\"/kaggle/working/test_tokenized_20k.csv\", index=False)\nvalid_df.to_csv(\"/kaggle/working/valid_tokenized_20k.csv\", index=False)\nprint(\"✅ Tokenized sentences saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:13:21.906190Z","iopub.execute_input":"2025-04-03T09:13:21.906519Z","iopub.status.idle":"2025-04-03T09:13:26.662085Z","shell.execute_reply.started":"2025-04-03T09:13:21.906495Z","shell.execute_reply":"2025-04-03T09:13:26.661280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 3: Load Pre-Trained Embeddings ===\nprint(\"📥 Loading pre-trained embeddings...\")\nword2vec_path = \"/kaggle/input/word2vec/GoogleNews-vectors-negative300.bin\"\nglove_path = \"/kaggle/input/glove-vectorisation/glove.6B.100d.txt\"\nfasttext_path = \"/kaggle/input/fasttext/cc.en.300.bin\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:46:54.819279Z","iopub.execute_input":"2025-04-13T04:46:54.819615Z","iopub.status.idle":"2025-04-13T04:46:54.824603Z","shell.execute_reply.started":"2025-04-13T04:46:54.819585Z","shell.execute_reply":"2025-04-13T04:46:54.823418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Load embeddings with progress tracking\nprint(\"📥 Loading Word2Vec...\")\nword2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\nprint(\"✅ Word2Vec loaded!\")\n\nfrom gensim.models.fasttext import load_facebook_model\n\nprint(\"📥 Loading FastText...\")\n\ntry:\n    # First, try loading as a standard Word2Vec format\n    fasttext = KeyedVectors.load_word2vec_format(fasttext_path, binary=True)\n    print(\"✅ FastText loaded as Word2Vec format!\")\n\nexcept UnicodeDecodeError:\n    print(\"⚠️ FastText binary loading failed! Trying Facebook FastText format...\")\n    \n    # Load FastText using Gensim's recommended method\n    fasttext = load_facebook_model(fasttext_path).wv  # Get word vectors\n    print(\"✅ FastText loaded successfully using Facebook format!\")\n\n# === Step 4: Speed Up Word Embedding Lookup Using FAISS and Multiprocessing ===\ndef create_faiss_index(embedding_model, embedding_dim):\n    \"\"\"Build FAISS index for fast nearest neighbor search.\"\"\"\n    index = faiss.IndexFlatL2(embedding_dim)  # Create FAISS index\n    words = embedding_model.index_to_key  # Get all words from KeyedVectors\n    vectors = np.array([embedding_model[word] for word in tqdm(words, desc=\"Indexing embeddings\")], dtype=np.float32)\n    \n    index.add(vectors)  # Add vectors to FAISS index\n    return index, words  # Return FAISS index & word list\n\nprint(\"🚀 Building FAISS indices for fast lookup...\")\nw2v_index, w2v_words = create_faiss_index(word2vec, 300)\nfasttext_index, fasttext_words = create_faiss_index(fasttext, 300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:53:12.332298Z","iopub.execute_input":"2025-04-13T04:53:12.333184Z","iopub.status.idle":"2025-04-13T04:56:55.318388Z","shell.execute_reply.started":"2025-04-13T04:53:12.333149Z","shell.execute_reply":"2025-04-13T04:56:55.317634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_faiss_embedding(word, embedding_dict, index, words_list, embedding_dim=300):\n    if word in embedding_dict:\n        return embedding_dict[word]\n    _, nearest = index.search(np.zeros((1, embedding_dim), dtype=np.float32), 1)\n    return embedding_dict[words_list[nearest[0][0]]]\n\n# Load GloVe embeddings using multiprocessing\nglove_embeddings = {}\nwith open(glove_path, 'r', encoding='utf-8') as f:\n    def process_line(line):\n        values = line.split()\n        return values[0], np.asarray(values[1:], dtype='float32')\n\n    with ProcessPoolExecutor() as executor:\n        results = list(tqdm(executor.map(process_line, f), desc=\"Loading GloVe in parallel\"))\n    \n    glove_embeddings = dict(results)\nprint(\"✅ GloVe loaded!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:58:55.055949Z","iopub.execute_input":"2025-04-13T04:58:55.056300Z","iopub.status.idle":"2025-04-13T05:01:12.037570Z","shell.execute_reply.started":"2025-04-13T04:58:55.056278Z","shell.execute_reply":"2025-04-13T05:01:12.036247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_and_save(df, prefix, chunk_size=1000):\n    \"\"\"Splits a dataframe into chunks and saves each as a separate file.\"\"\"\n    total_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size != 0 else 0)\n\n    for i in range(total_chunks):\n        start = i * chunk_size\n        end = min((i + 1) * chunk_size, len(df))\n        chunk_df = df.iloc[start:end]\n        chunk_df.to_csv(f\"{prefix}_chunk_{i+1}.csv\", index=False)\n        print(f\"✅ Saved {prefix}_chunk_{i+1}.csv\")\n\n# Split & save training, validation, and test sets\nsplit_and_save(train_df, \"train\")\nsplit_and_save(test_df, \"test\")\nsplit_and_save(valid_df, \"val\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T09:34:56.853425Z","iopub.execute_input":"2025-04-03T09:34:56.853774Z","iopub.status.idle":"2025-04-03T09:35:01.815993Z","shell.execute_reply.started":"2025-04-03T09:34:56.853745Z","shell.execute_reply":"2025-04-03T09:35:01.815177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom tqdm.auto import tqdm\n\n# === Step 1: Check & Set Device ===\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"🚀 Using device: {device}\")\n\n# === Step 2: Move Word2Vec Model to GPU ===\nword2vec_vectors = torch.tensor(word2vec.vectors, dtype=torch.float16, device=device)  # Move embeddings to GPU\nword2vec_vocab = word2vec.index_to_key\nword2idx = {word: idx for idx, word in enumerate(word2vec_vocab)}\n\n# === Step 3: Move GloVe & FastText to GPU ===\nglove_vocab = list(glove_embeddings.keys())\nglove_vectors = torch.tensor([glove_embeddings[word] for word in glove_vocab], dtype=torch.float16, device=device)\nglove_word2idx = {word: idx for idx, word in enumerate(glove_vocab)}\n\nfasttext_vocab = fasttext.index_to_key\nfasttext_vectors = torch.tensor(fasttext.vectors, dtype=torch.float16, device=device)\nfasttext_word2idx = {word: idx for idx, word in enumerate(fasttext_vocab)}\n\n# === Step 4: Optimized Word Embedding Lookup on GPU ===\ndef get_embedding(word, word2idx, embedding_vectors):\n    \"\"\"Retrieve word embedding from GPU tensors.\"\"\"\n    idx = word2idx.get(word, None)\n    if idx is not None:\n        return embedding_vectors[idx]  # GPU lookup\n    return torch.zeros_like(embedding_vectors[0])  # Zero vector for unknown words\n\n# === Step 5: Convert Sentences to Word Embeddings Using Mini-Batches ===\nMAX_SENTENCES = 50\nMAX_WORDS = 50\nEMBEDDING_DIM_W2V = 300\nEMBEDDING_DIM_GLOVE = 100\nEMBEDDING_DIM_FASTTEXT = 300\nBATCH_SIZE = 500  # 🚀 Process in batches to avoid OOM\n\ndef sentence_to_vector(sentence, word2idx, embedding_vectors, embedding_dim, max_words=MAX_WORDS):\n    \"\"\"Convert a sentence into a GPU-accelerated word embedding matrix.\"\"\"\n    words = sentence.split()[:max_words]\n    embedding_matrix = torch.zeros((max_words, embedding_dim), dtype=torch.float16, device=device)\n\n    for i, word in enumerate(words):\n        embedding_matrix[i] = get_embedding(word, word2idx, embedding_vectors)\n\n    return embedding_matrix\n\ndef article_to_vectors(sentences, word2idx, embedding_vectors, embedding_dim, max_sentences=MAX_SENTENCES):\n    \"\"\"Convert all sentences in an article to a padded 3D tensor.\"\"\"\n    sentence_vectors = [sentence_to_vector(sent, word2idx, embedding_vectors, embedding_dim) for sent in sentences]\n\n    # Pad or truncate to MAX_SENTENCES\n    num_sentences = len(sentence_vectors)\n    if num_sentences < max_sentences:\n        padding = [torch.zeros((MAX_WORDS, embedding_dim), dtype=torch.float16, device=device)] * (max_sentences - num_sentences)\n        return torch.stack(sentence_vectors + padding)\n\n    return torch.stack(sentence_vectors[:max_sentences])\n\n# === Step 6: Process in Mini-Batches ===\ndef process_in_batches(df, word2idx, embedding_vectors, embedding_dim, batch_size=BATCH_SIZE):\n    \"\"\"Process dataset in small batches to prevent GPU memory overflow.\"\"\"\n    total_samples = len(df)\n    all_vectors = []\n\n    for start in tqdm(range(0, total_samples, batch_size), desc=\"🚀 Processing batches on GPU\"):\n        end = min(start + batch_size, total_samples)\n        batch_df = df.iloc[start:end]  # Select batch\n\n        batch_vectors = [\n            article_to_vectors(sentences, word2idx, embedding_vectors, embedding_dim)\n            for sentences in batch_df[\"sentences\"]\n        ]\n\n        batch_vectors = torch.stack(batch_vectors).cpu()  # Move to CPU to free GPU memory\n        all_vectors.append(batch_vectors)\n        \n        torch.cuda.empty_cache()  # 🚀 Free GPU memory after each batch\n\n    return torch.cat(all_vectors)  # Combine all batches\n\n\n# === Step 7: Process All Files in Directory & Save Output ===\ninput_dir = \"/kaggle/input/train-chunk\"\noutput_dir = \"/kaggle/working\"\n\nskip_files = {\n    \"train_chunk_20\", \"train_chunk_3\",\"train_chunk_1\",\"train_chunk_4\",\"train_chunk_15\",\"train_chunk_19\",\n    \"train_chunk_17\",\"train_chunk_6\",\"train_chunk_9\",\"train_chunk_12\",\"train_chunk_5\",\n    \"train_chunk_7\",\"train_chunk_2\"\n}\n\nprocessed_files = {f.replace('.pt', '') for f in os.listdir(output_dir) if f.endswith(\".pt\")}\n\nfor file_name in os.listdir(input_dir):\n    if file_name.endswith(\".csv\"):\n        base_name = file_name.replace('.csv', '')\n\n        # Skip if already processed OR explicitly listed in skip_files\n        if base_name in processed_files or base_name in skip_files:\n            print(f\"⏩ Skipping: {file_name}\")\n            continue\n\n        file_path = os.path.join(input_dir, file_name)\n        df = pd.read_csv(file_path)\n\n        print(f\"📂 Processing {file_name} ...\")\n        processed_data = process_in_batches(df, word2idx, word2vec_vectors, EMBEDDING_DIM_W2V)\n\n        # Save output\n        output_file = os.path.join(output_dir, f\"{base_name}.pt\")\n        torch.save(processed_data, output_file)\n        print(f\"✅ Saved {output_file}\")\n\nprint(f\"🎉 Remaining files processed and saved in {output_dir}!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T11:05:40.283958Z","iopub.execute_input":"2025-04-04T11:05:40.284296Z","iopub.status.idle":"2025-04-04T11:21:05.235618Z","shell.execute_reply.started":"2025-04-04T11:05:40.284272Z","shell.execute_reply":"2025-04-04T11:21:05.234689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 0: Remove Existing .pt Files from Output Directory ===\nfor f in os.listdir(output_dir):\n    if f.endswith(\".pt\"):\n        os.remove(os.path.join(output_dir, f))\nprint(\"🧹 Cleared previous .pt files from working directory.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:36:32.777884Z","iopub.execute_input":"2025-04-04T10:36:32.778287Z","iopub.status.idle":"2025-04-04T10:36:33.517661Z","shell.execute_reply.started":"2025-04-04T10:36:32.778257Z","shell.execute_reply":"2025-04-04T10:36:33.516785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom tqdm.auto import tqdm\n\n# === Step 1: Check & Set Device ===\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"🚀 Using device: {device}\")\n\n# === Step 2: Move Word2Vec Model to GPU ===\nword2vec_vectors = torch.tensor(word2vec.vectors, dtype=torch.float16, device=device)  # Move embeddings to GPU\nword2vec_vocab = word2vec.index_to_key\nword2idx = {word: idx for idx, word in enumerate(word2vec_vocab)}\n\n# === Step 3: Move GloVe & FastText to GPU ===\nglove_vocab = list(glove_embeddings.keys())\nglove_vectors = torch.tensor([glove_embeddings[word] for word in glove_vocab], dtype=torch.float16, device=device)\nglove_word2idx = {word: idx for idx, word in enumerate(glove_vocab)}\n\nfasttext_vocab = fasttext.index_to_key\nfasttext_vectors = torch.tensor(fasttext.vectors, dtype=torch.float16, device=device)\nfasttext_word2idx = {word: idx for idx, word in enumerate(fasttext_vocab)}\n\n# === Step 4: Optimized Word Embedding Lookup on GPU ===\ndef get_embedding(word, word2idx, embedding_vectors):\n    \"\"\"Retrieve word embedding from GPU tensors.\"\"\"\n    idx = word2idx.get(word, None)\n    if idx is not None:\n        return embedding_vectors[idx]  # GPU lookup\n    return torch.zeros_like(embedding_vectors[0])  # Zero vector for unknown words\n\n# === Step 5: Convert Sentences to Word Embeddings Using Mini-Batches ===\nMAX_SENTENCES = 50\nMAX_WORDS = 50\nEMBEDDING_DIM_W2V = 300\nEMBEDDING_DIM_GLOVE = 100\nEMBEDDING_DIM_FASTTEXT = 300\nBATCH_SIZE = 500  # 🚀 Process in batches to avoid OOM\n\ndef sentence_to_vector(sentence, word2idx, embedding_vectors, embedding_dim, max_words=MAX_WORDS):\n    \"\"\"Convert a sentence into a GPU-accelerated word embedding matrix.\"\"\"\n    words = sentence.split()[:max_words]\n    embedding_matrix = torch.zeros((max_words, embedding_dim), dtype=torch.float16, device=device)\n\n    for i, word in enumerate(words):\n        embedding_matrix[i] = get_embedding(word, word2idx, embedding_vectors)\n\n    return embedding_matrix\n\ndef article_to_vectors(sentences, word2idx, embedding_vectors, embedding_dim, max_sentences=MAX_SENTENCES):\n    \"\"\"Convert all sentences in an article to a padded 3D tensor.\"\"\"\n    sentence_vectors = [sentence_to_vector(sent, word2idx, embedding_vectors, embedding_dim) for sent in sentences]\n\n    # Pad or truncate to MAX_SENTENCES\n    num_sentences = len(sentence_vectors)\n    if num_sentences < max_sentences:\n        padding = [torch.zeros((MAX_WORDS, embedding_dim), dtype=torch.float16, device=device)] * (max_sentences - num_sentences)\n        return torch.stack(sentence_vectors + padding)\n\n    return torch.stack(sentence_vectors[:max_sentences])\n\n# === Step 6: Process in Mini-Batches ===\ndef process_in_batches(df, word2idx, embedding_vectors, embedding_dim, batch_size=BATCH_SIZE):\n    \"\"\"Process dataset in small batches to prevent GPU memory overflow.\"\"\"\n    total_samples = len(df)\n    all_vectors = []\n\n    for start in tqdm(range(0, total_samples, batch_size), desc=\"🚀 Processing batches on GPU\"):\n        end = min(start + batch_size, total_samples)\n        batch_df = df.iloc[start:end]  # Select batch\n\n        batch_vectors = [\n            article_to_vectors(sentences, word2idx, embedding_vectors, embedding_dim)\n            for sentences in batch_df[\"sentences\"]\n        ]\n\n        batch_vectors = torch.stack(batch_vectors).cpu()  # Move to CPU to free GPU memory\n        all_vectors.append(batch_vectors)\n        \n        torch.cuda.empty_cache()  # 🚀 Free GPU memory after each batch\n\n    return torch.cat(all_vectors)  # Combine all batches\n\n\n# === Step 7: Loop over All CSVs and Save to Output Folder ===\ninput_dir = \"/kaggle/input/train-chunk/\"\noutput_dir = \"/kaggle/working/train_chunk_pt/\"\nos.makedirs(output_dir, exist_ok=True)\n\nprint(\"🔄 Starting GloVe embedding conversion for all chunks...\")\nfor file_name in sorted(os.listdir(input_dir)):\n    if file_name.endswith(\".csv\"):\n        base_name = file_name.replace(\".csv\", \"\")\n        input_path = os.path.join(input_dir, file_name)\n        output_path = os.path.join(output_dir, f\"{base_name}_glove.pt\")\n\n        print(f\"📂 Processing {file_name} ...\")\n        df = pd.read_csv(input_path)\n\n        # Process using GloVe\n        processed_tensor = process_in_batches(df, glove_word2idx, glove_vectors, EMBEDDING_DIM_GLOVE)\n\n        # Save the tensor\n        torch.save(processed_tensor, output_path)\n        print(f\"✅ Saved {output_path}\")\n\nprint(\"🎉 All chunk files processed and saved in:\", output_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T09:39:25.731316Z","iopub.execute_input":"2025-04-08T09:39:25.731650Z","iopub.status.idle":"2025-04-08T10:18:27.942645Z","shell.execute_reply.started":"2025-04-08T09:39:25.731626Z","shell.execute_reply":"2025-04-08T10:18:27.941694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport os\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# === Step 1: Check & Set Device ===\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"🚀 Using device: {device}\")\n\n# === Step 2: Move Word2Vec Model to GPU ===\nword2vec_vectors = torch.tensor(word2vec.vectors, dtype=torch.float16, device=device)\nword2vec_vocab = word2vec.index_to_key\nword2idx = {word: idx for idx, word in enumerate(word2vec_vocab)}\n\n# === Step 3: Move GloVe & FastText to GPU ===\nglove_vocab = list(glove_embeddings.keys())\nglove_vectors = torch.tensor([glove_embeddings[word] for word in glove_vocab], dtype=torch.float16, device=device)\nglove_word2idx = {word: idx for idx, word in enumerate(glove_vocab)}\n\nfasttext_vocab = fasttext.index_to_key\nfasttext_vectors = torch.tensor(fasttext.vectors, dtype=torch.float16, device=device)\nfasttext_word2idx = {word: idx for idx, word in enumerate(fasttext_vocab)}\n\n# === Step 4: Optimized Word Embedding Lookup on GPU ===\ndef get_embedding(word, word2idx, embedding_vectors):\n    idx = word2idx.get(word, None)\n    if idx is not None:\n        return embedding_vectors[idx]\n    return torch.zeros_like(embedding_vectors[0])\n\n# === Step 5: Convert Sentences to Word Embeddings Using Mini-Batches ===\nMAX_SENTENCES = 50\nMAX_WORDS = 50\nEMBEDDING_DIM_W2V = 300\nEMBEDDING_DIM_GLOVE = 100\nEMBEDDING_DIM_FASTTEXT = 300\nBATCH_SIZE = 500\n\ndef sentence_to_vector(sentence, word2idx, embedding_vectors, embedding_dim, max_words=MAX_WORDS):\n    words = sentence.split()[:max_words]\n    embedding_matrix = torch.zeros((max_words, embedding_dim), dtype=torch.float16, device=device)\n    for i, word in enumerate(words):\n        embedding_matrix[i] = get_embedding(word, word2idx, embedding_vectors)\n    return embedding_matrix\n\ndef article_to_vectors(sentences, word2idx, embedding_vectors, embedding_dim, max_sentences=MAX_SENTENCES):\n    sentence_vectors = [sentence_to_vector(sent, word2idx, embedding_vectors, embedding_dim) for sent in sentences]\n    num_sentences = len(sentence_vectors)\n    if num_sentences < max_sentences:\n        padding = [torch.zeros((MAX_WORDS, embedding_dim), dtype=torch.float16, device=device)] * (max_sentences - num_sentences)\n        return torch.stack(sentence_vectors + padding)\n    return torch.stack(sentence_vectors[:max_sentences])\n\ndef process_in_batches(df, word2idx, embedding_vectors, embedding_dim, batch_size=BATCH_SIZE):\n    total_samples = len(df)\n    all_vectors = []\n    for start in tqdm(range(0, total_samples, batch_size), desc=\"🚀 Processing batches on GPU\"):\n        end = min(start + batch_size, total_samples)\n        batch_df = df.iloc[start:end]\n        batch_vectors = [\n            article_to_vectors(sentences, word2idx, embedding_vectors, embedding_dim)\n            for sentences in batch_df[\"sentences\"]\n        ]\n        batch_vectors = torch.stack(batch_vectors).cpu()\n        all_vectors.append(batch_vectors)\n        torch.cuda.empty_cache()\n    return torch.cat(all_vectors)\n\n# === NEW: Process all files in the folder except specified ===\nprint(\"🔢 Converting FastText embeddings for selected chunks...\")\ninput_folder = \"/kaggle/input/train-chunk\"\noutput_folder = \"/kaggle/working/train_fasttext_chunks\"\nos.makedirs(output_folder, exist_ok=True)\n\n# Skip these files\nskip_files = {\n    \"train_chunk_1.csv\", \"train_chunk_10.csv\", \"train_chunk_11.csv\",\n    \"train_chunk_12.csv\", \"train_chunk_13.csv\", \"train_chunk_14.csv\",\n    \"train_chunk_15.csv\", \"train_chunk_16.csv\", \"train_chunk_17.csv\",\n    \"train_chunk_18.csv\", \"train_chunk_19.csv\", \"train_chunk_2.csv\"\n}\n\nfor file in sorted(os.listdir(input_folder)):\n    if file.endswith(\".csv\") and file not in skip_files:\n        chunk_path = os.path.join(input_folder, file)\n        print(f\"📄 Processing: {file}\")\n        df = pd.read_csv(chunk_path)\n        df[\"sentences\"] = df[\"sentences\"].apply(eval)\n        \n        chunk_vectors = process_in_batches(df, fasttext_word2idx, fasttext_vectors, EMBEDDING_DIM_FASTTEXT)\n        \n        save_name = os.path.splitext(file)[0] + \"_fasttext.pt\"\n        torch.save(chunk_vectors, os.path.join(output_folder, save_name))\n        print(f\"✅ Saved: {save_name}\")\n\nprint(\"🎉 Selected FastText chunks processed and saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T07:19:40.528182Z","iopub.execute_input":"2025-04-11T07:19:40.528493Z","iopub.status.idle":"2025-04-11T07:21:57.023347Z","shell.execute_reply.started":"2025-04-11T07:19:40.528470Z","shell.execute_reply":"2025-04-11T07:21:57.022266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 0: Remove Existing .pt Files from Output Directory ===\noutput_dir = \"/kaggle/working/train_fasttext_chunks/\"\nfor f in os.listdir(output_dir):\n    if f.endswith(\".pt\"):\n        os.remove(os.path.join(output_dir, f))\nprint(\"🧹 Cleared previous .pt files from working directory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T07:19:09.360461Z","iopub.execute_input":"2025-04-11T07:19:09.360759Z","iopub.status.idle":"2025-04-11T07:19:10.256653Z","shell.execute_reply.started":"2025-04-11T07:19:09.360737Z","shell.execute_reply":"2025-04-11T07:19:10.255613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\n\n# List of directories to scan\ndirectories = [\n     \"/kaggle/input/word2vec-train-10-chunks\",\n     \"/kaggle/input/word2vec-train-chunks-10-pt2\",\n     \"/kaggle/input/glove-train-chunk-pt1\",\n     \"/kaggle/input/glove-train-chunk-pt2\",\n     \"/kaggle/input/fasttext-train-chunk-pt1\",\n     \"/kaggle/input/fasttext-train-chunk-pt2\"\n]\n\nfor dir_path in directories:\n    print(f\"\\n📁 Directory: {dir_path}\")\n    for file in sorted(os.listdir(dir_path)):\n        if file.endswith(\".pt\"):\n            file_path = os.path.join(dir_path, file)\n            try:\n                tensor = torch.load(file_path, map_location=\"cpu\")\n                print(f\"  📄 {file}: shape = {tensor.shape}\")\n            except Exception as e:\n                print(f\"  ❌ Could not load {file}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T06:02:27.046775Z","iopub.execute_input":"2025-04-13T06:02:27.047137Z","iopub.status.idle":"2025-04-13T06:09:46.528279Z","shell.execute_reply.started":"2025-04-13T06:02:27.047108Z","shell.execute_reply":"2025-04-13T06:09:46.527309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom rouge_score import rouge_scorer\nfrom tqdm import tqdm\nimport os\n\n# class LazyEmbeddingDataset(Dataset):\n#     def __init__(self, directories):\n#         self.file_paths = []\n#         self.chunk_sizes = []\n        \n\n#         # List all .pt files and store their sample counts\n#         for directory in directories:\n#             for file in sorted(os.listdir(directory)):\n#                 if file.endswith(\".pt\"):\n#                     full_path = os.path.join(directory, file)\n#                     data = torch.load(full_path, map_location=\"cpu\")\n#                     self.file_paths.append(full_path)\n#                     self.chunk_sizes.append(data.shape[0])  # Assume first dim is sample count\n\n#         # Build index mapping: global_index -> (file_idx, local_index)\n#         self.index_map = []\n#         for file_idx, size in enumerate(self.chunk_sizes):\n#             for local_idx in range(size):\n#                 self.index_map.append((file_idx, local_idx))\n\n#     def __len__(self):\n#         return len(self.index_map)\n\n#     def __getitem__(self, idx):\n#         file_idx, local_idx = self.index_map[idx]\n#         file_path = self.file_paths[file_idx]\n#         data = torch.load(file_path, map_location=\"cpu\")\n#         return data[local_idx]\n\n# # Define your folder lists\n# word2vec_dirs = [\n#     \"/kaggle/input/word2vec-train-10-chunks\",\n#     \"/kaggle/input/word2vec-train-chunks-10-pt2\"\n# ]\n\n# glove_dirs = [\n#     \"/kaggle/input/glove-train-chunk-pt1\",\n#     \"/kaggle/input/glove-train-chunk-pt2\"\n# ]\n\n# # fasttext_dirs = [\n# #     \"/kaggle/input/fasttext-train-chunk-pt1\"\n# # ]\n\n# # Instantiate datasets (lazy)\n# word2vec_dataset = LazyEmbeddingDataset(word2vec_dirs)\n# glove_dataset = LazyEmbeddingDataset(glove_dirs)\n# # fasttext_dataset = LazyEmbeddingDataset(fasttext_dirs)\n\n# # Example DataLoader (batch size = 16, shuffle = True)\n# word2vec_loader = DataLoader(word2vec_dataset, batch_size=16, shuffle=True)\n# glove_loader = DataLoader(glove_dataset, batch_size=16, shuffle=True)\n# # fasttext_loader = DataLoader(fasttext_dataset, batch_size=16, shuffle=True)\n\n# # Example: iterate\n# # for batch in word2vec_loader:\n# #     batch = batch.to(\"cuda\")\n# #     # your training logic\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:30:03.052779Z","iopub.execute_input":"2025-04-13T04:30:03.053251Z","iopub.status.idle":"2025-04-13T04:30:06.655895Z","shell.execute_reply.started":"2025-04-13T04:30:03.053213Z","shell.execute_reply":"2025-04-13T04:30:06.655250Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"class TextSummaryDataset(Dataset):\n    def __init__(self, embeddings):\n        self.embeddings = embeddings.cpu()  # Ensure embeddings are on CPU\n\n    def __len__(self):\n        return len(self.embeddings)\n\n    def __getitem__(self, idx):\n        x = self.embeddings[idx]  # Shape: (50, 50, 300)\n        #print(f\"Shape before mean pooling: {x.shape}\")  # Debugging\n\n        x = x.mean(dim=1)  # ✅ Mean pooling over words (50, 300)\n        #print(f\"Shape after mean pooling: {x.shape}\")  # Debugging\n        \n        return torch.tensor(x, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:33:15.171390Z","iopub.execute_input":"2025-04-13T04:33:15.171957Z","iopub.status.idle":"2025-04-13T04:33:15.176780Z","shell.execute_reply.started":"2025-04-13T04:33:15.171901Z","shell.execute_reply":"2025-04-13T04:33:15.175946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class WL_AttenSumm(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim=256, num_filters=100, filter_sizes=[1,2,3,4,5,6,7]):\n        super(WL_AttenSumm, self).__init__()\n        \n        # Convolutional Layers with multiple filter sizes\n        self.convs = nn.ModuleList([\n            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs, padding=fs//2)\n            for fs in filter_sizes\n        ])\n        \n        # Bi-GRU Layer\n        self.gru = nn.GRU(input_size=num_filters * len(filter_sizes), hidden_size=hidden_dim, \n                          bidirectional=True, batch_first=True)\n        \n        # Word-level Attention Layer\n        self.attention = nn.Linear(hidden_dim * 2, 1)  # Computes attention scores\n\n        # Fully connected layer for sentence scoring\n        self.fc = nn.Linear(hidden_dim * 2, 1)  # Uses context vector g_t\n\n    def forward(self, x):\n        #print(f\"Before permute: {x.shape}\")  # Debugging\n        x = x.permute(0, 2, 1)  # ✅ Fix: Change (batch_size, seq_len, embedding_dim) to (batch_size, embedding_dim, seq_len)\n        #print(f\"After permute: {x.shape}\")  # Debugging\n        \n        # Apply multiple CNN filters\n        conv_outputs = [F.relu(conv(x)) for conv in self.convs]  # List of (batch, num_filters, seq_len)\n        \n        # Max-pooling over time for each convolution output\n        pooled_outputs = [F.max_pool1d(co, kernel_size=co.shape[2]).squeeze(2) for co in conv_outputs]  # (batch, num_filters)\n        \n        # Concatenate all pooled features\n        cnn_output = torch.cat(pooled_outputs, dim=1)  # (batch, num_filters * len(filter_sizes))\n        \n        # Expand back to sequence shape for Bi-GRU\n        x = cnn_output.unsqueeze(1).expand(-1, 50, -1)  # (batch, seq_len, num_filters * len(filter_sizes))\n        \n        # Bi-GRU\n        x, _ = self.gru(x)  # Output shape: (batch, seq_len, hidden_dim*2)\n        \n        # Word-Level Attention Mechanism\n        attn_weights = torch.softmax(self.attention(x).squeeze(-1), dim=1)  # Compute attention weights (batch, seq_len)\n        gt = torch.sum(attn_weights.unsqueeze(-1) * x, dim=1)  # Compute weighted sum of Bi-GRU outputs\n        \n        # Fully connected MLP to compute sentence scores\n        yi = torch.sigmoid(self.fc(x)).squeeze(-1)  # ✅ Ensure this outputs (batch_size, seq_len)\n        \n        return yi  # ✅ Shape should be (batch_size, 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:33:17.933390Z","iopub.execute_input":"2025-04-13T04:33:17.933725Z","iopub.status.idle":"2025-04-13T04:33:17.942257Z","shell.execute_reply.started":"2025-04-13T04:33:17.933698Z","shell.execute_reply":"2025-04-13T04:33:17.941506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Should print True\nprint(torch.cuda.device_count())  # Should print number of GPUs\nprint(torch.cuda.get_device_name(0))  # Prints GPU name\ntorch.cuda.current_device()  # check which GPU\ntorch.cuda.memory_allocated() / 1024**2  # in MB\ntorch.cuda.memory_reserved() / 1024**2  # in MB","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:33:21.600756Z","iopub.execute_input":"2025-04-13T04:33:21.601075Z","iopub.status.idle":"2025-04-13T04:33:21.734122Z","shell.execute_reply.started":"2025-04-13T04:33:21.601051Z","shell.execute_reply":"2025-04-13T04:33:21.733386Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 20k Word2Vec","metadata":{}},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"# Function to train model on a single file\ndef train_on_file(model, file_path, device, epochs=20):\n    print(f\"\\nTraining on file: {os.path.basename(file_path)}\")\n    \n    # Load embeddings from file to CPU first\n    embeddings = torch.load(file_path, map_location=\"cpu\")\n    \n    # Create dataset and dataloader\n    dataset = TextSummaryDataset(embeddings)\n    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n    \n    # Set up optimizer\n    optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.99, 0.999))\n    \n    # Training loop for this file\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n        for inputs in progress:\n            inputs = inputs.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            \n            # Compute loss\n            loss = torch.mean((outputs - outputs.mean(dim=1, keepdim=True)) ** 2)\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            progress.set_postfix(loss=running_loss / len(dataloader))\n        \n        print(f\"File: {os.path.basename(file_path)} - Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.4f}\")\n    \n    # Clear memory to avoid OOM errors\n    del embeddings, dataset, dataloader\n    torch.cuda.empty_cache()\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:39:59.849075Z","iopub.execute_input":"2025-04-12T06:39:59.849467Z","iopub.status.idle":"2025-04-12T06:39:59.856689Z","shell.execute_reply.started":"2025-04-12T06:39:59.849434Z","shell.execute_reply":"2025-04-12T06:39:59.855559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef main():\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Get all the training files\n    files_part1 = sorted(glob.glob(\"/kaggle/input/word2vec-train-10-chunks/*.pt\"))\n    files_part2 = sorted(glob.glob(\"/kaggle/input/word2vec-train-chunks-10-pt2/*.pt\"))\n    all_files = files_part1 + files_part2\n    \n    print(f\"Found {len(all_files)} training files\")\n    \n    # Check if there's a checkpoint to continue from\n    checkpoint_path = \"model_checkpoint.pt\"\n    start_file_idx = 0\n    \n    if os.path.exists(checkpoint_path):\n        print(f\"Loading checkpoint from {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path)\n        model = checkpoint['model']\n        start_file_idx = checkpoint['file_idx'] + 1\n        print(f\"Continuing from file index {start_file_idx}\")\n    else:\n        # Initialize model\n        # Load first file just to get embedding_dim\n        sample_data = torch.load(all_files[0], map_location=\"cpu\")\n        embedding_dim = sample_data.shape[-1]  # Should be 300 for Word2Vec\n        del sample_data  # Free memory\n        \n        model = WL_AttenSumm(embedding_dim=embedding_dim)\n        print(f\"Initialized new model with embedding_dim={embedding_dim}\")\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Train on each file sequentially\n    for i, file_path in enumerate(all_files[start_file_idx:], start=start_file_idx):\n        print(f\"\\nProcessing file {i+1}/{len(all_files)}: {os.path.basename(file_path)}\")\n        \n        # Train model on this file\n        model = train_on_file(model, file_path, device, epochs=20)\n        \n        # Save checkpoint after each file\n        checkpoint = {\n            'model': model.cpu(),  # Save model to CPU to avoid CUDA memory issues\n            'file_idx': i\n        }\n        torch.save(checkpoint, checkpoint_path)\n        \n        # Move model back to device for next training\n        model = model.to(device)\n        \n        print(f\"Saved checkpoint after file {i+1}/{len(all_files)}\")\n    \n    # Save final model\n    torch.save(model.cpu(), \"final_model.pt\")\n    print(\"Training completed on all files. Final model saved.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T06:40:20.908030Z","iopub.execute_input":"2025-04-12T06:40:20.908431Z","iopub.status.idle":"2025-04-12T06:47:27.121871Z","shell.execute_reply.started":"2025-04-12T06:40:20.908396Z","shell.execute_reply":"2025-04-12T06:47:27.121150Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Testing\n","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nimport re\nfrom rouge_score import rouge_scorer\n\n# ===== Step 1: Load Word Embeddings =====\nprint(\"Loading word embeddings...\")\n\n# Load your word2vec model - modify this to match how you loaded it originally\n# This assumes you have already loaded these models elsewhere\n# If not, you'll need to load them first\nword2vec_vectors = torch.tensor(word2vec.vectors, dtype=torch.float16)  # CPU first\nword2vec_vocab = word2vec.index_to_key\nword2idx = {word: idx for idx, word in enumerate(word2vec_vocab)}\n\n# ===== Step 2: Text Processing Functions =====\ndef preprocess_text(text):\n    \"\"\"Clean and preprocess text for embedding.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Remove special characters and numbers, keep only alphabets and spaces\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n    \n    # Replace multiple spaces with single space\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text.lower().strip()\n\ndef split_into_sentences(text):\n    \"\"\"Split text into sentences.\"\"\"\n    if not isinstance(text, str):\n        return []\n    \n    # Basic sentence splitting on punctuation followed by space\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    return [s.strip() for s in sentences if s.strip()]\n\n# ===== Step 3: Embedding Generation Functions =====\nMAX_SENTENCES = 50\nMAX_WORDS = 50\nEMBEDDING_DIM = 300  # Word2Vec dimension\n\ndef get_embedding(word, word2idx, embedding_vectors):\n    \"\"\"Retrieve word embedding.\"\"\"\n    idx = word2idx.get(word, None)\n    if idx is not None:\n        return embedding_vectors[idx]\n    return torch.zeros_like(embedding_vectors[0])  # Zero vector for unknown words\n\ndef sentence_to_vector(sentence, word2idx, embedding_vectors, max_words=MAX_WORDS):\n    \"\"\"Convert a sentence into a word embedding matrix.\"\"\"\n    words = preprocess_text(sentence).split()[:max_words]\n    embedding_matrix = torch.zeros((max_words, EMBEDDING_DIM), dtype=torch.float16)\n\n    for i, word in enumerate(words):\n        embedding_matrix[i] = get_embedding(word, word2idx, embedding_vectors)\n\n    return embedding_matrix\n\ndef article_to_vectors(sentences, word2idx, embedding_vectors, max_sentences=MAX_SENTENCES):\n    \"\"\"Convert all sentences in an article to a padded 3D tensor.\"\"\"\n    sentence_vectors = [\n        sentence_to_vector(sent, word2idx, embedding_vectors) \n        for sent in sentences[:max_sentences]\n    ]\n\n    # Pad if necessary\n    num_sentences = len(sentence_vectors)\n    if num_sentences < max_sentences:\n        padding = [torch.zeros((MAX_WORDS, EMBEDDING_DIM), dtype=torch.float16)] * (max_sentences - num_sentences)\n        return torch.stack(sentence_vectors + padding)\n\n    return torch.stack(sentence_vectors[:max_sentences])\n\n# ===== Step 4: Process Test Dataset =====\ndef process_test_dataset(df_path, word2idx, embedding_vectors, batch_size=100):\n    \"\"\"Process test dataset and convert to embeddings.\"\"\"\n    print(f\"Loading test dataset from {df_path}...\")\n    df = pd.read_csv(df_path)\n    \n    print(f\"Processing {len(df)} test articles...\")\n    all_vectors = []\n    original_articles = []\n    highlights = []\n\n    # Process in batches to manage memory\n    for start_idx in tqdm(range(0, len(df), batch_size), desc=\"Processing test batches\"):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_df = df.iloc[start_idx:end_idx]\n        \n        batch_vectors = []\n        for article in batch_df[\"article\"]:\n            sentences = split_into_sentences(article)\n            article_vectors = article_to_vectors(sentences, word2idx, embedding_vectors)\n            batch_vectors.append(article_vectors)\n            \n        # Store original articles and highlights for later use\n        original_articles.extend(batch_df[\"article\"].tolist())\n        highlights.extend(batch_df[\"highlights\"].tolist())\n        \n        # Stack batch and add to result\n        if batch_vectors:\n            all_vectors.append(torch.stack(batch_vectors))\n    \n    # Combine all batches\n    test_embeddings = torch.cat(all_vectors)\n    print(f\"Generated embeddings shape: {test_embeddings.shape}\")\n    \n    return test_embeddings, original_articles, highlights\n\n# ===== Step 5: TextSummaryDataset Class =====\nclass TextSummaryDataset(Dataset):\n    def __init__(self, embeddings):\n        self.embeddings = embeddings.cpu()  # Ensure embeddings are on CPU\n\n    def __len__(self):\n        return len(self.embeddings)\n\n    def __getitem__(self, idx):\n        x = self.embeddings[idx]  # Shape: (50, 50, 300)\n        x = x.mean(dim=1)  # Mean pooling over words (50, 300)\n        return torch.tensor(x, dtype=torch.float32)\n\n# ===== Step 6: Summary Extraction and Evaluation =====\ndef extract_top_sentences(scores, articles, top_k=3):\n    \"\"\"Extract top-k sentences from each article based on scores.\"\"\"\n    summaries = []\n    \n    for i, article in enumerate(articles):\n        sentences = split_into_sentences(article)\n        \n        if not sentences:\n            summaries.append(\"\")\n            continue\n            \n        # Get actual number of sentences\n        num_sentences = min(len(sentences), scores.shape[1])\n        \n        # Get scores for this article\n        article_scores = scores[i][:num_sentences]\n        \n        # Get indices of top-k sentences\n        if len(article_scores) <= top_k:\n            top_indices = np.arange(len(article_scores))\n        else:\n            top_indices = np.argsort(-article_scores)[:top_k]\n        \n        # Sort indices to maintain original order\n        top_indices = sorted(top_indices)\n        \n        # Extract selected sentences\n        selected_sentences = [sentences[idx] for idx in top_indices if idx < len(sentences)]\n        \n        # Join sentences\n        summary = \" \".join(selected_sentences)\n        summaries.append(summary)\n    \n    return summaries\n\ndef compute_rouge(predicted_summaries, gold_summaries):\n    \"\"\"Compute ROUGE scores.\"\"\"\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n    \n    for pred, gold in zip(predicted_summaries, gold_summaries):\n        # Handle empty strings\n        if not isinstance(pred, str) or not pred.strip():\n            pred = \"empty summary\"\n        if not isinstance(gold, str) or not gold.strip():\n            gold = \"empty summary\"\n            \n        score = scorer.score(gold, pred)\n        \n        for key in scores:\n            scores[key].append(score[key].fmeasure)\n    \n    # Calculate average scores\n    avg_scores = {key: np.mean(val) for key, val in scores.items()}\n    \n    return avg_scores\n\n# ===== Step 7: Main Evaluation Function =====\ndef evaluate_model(model_path, test_data_path, word2idx, embedding_vectors):\n    \"\"\"End-to-end evaluation pipeline.\"\"\"\n    print(f\"Starting evaluation pipeline...\")\n    \n    # Step 1: Load model\n    print(f\"Loading model from {model_path}...\")\n    model = torch.load(model_path)\n    \n    # Step 2: Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    model = model.to(device)\n    \n    # Step 3: Generate test embeddings\n    print(\"Generating test embeddings...\")\n    test_embeddings, original_articles, highlights = process_test_dataset(\n        test_data_path, word2idx, embedding_vectors\n    )\n    \n    # Step 4: Save embeddings to avoid regenerating (optional)\n    torch.save(test_embeddings, \"test_embeddings.pt\")\n    print(\"Saved test embeddings to test_embeddings.pt\")\n    \n    # Step 5: Create dataset and dataloader\n    test_dataset = TextSummaryDataset(test_embeddings)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n    \n    # Step 6: Extract sentence scores\n    model.eval()\n    all_scores = []\n    \n    print(\"Generating sentence scores...\")\n    with torch.no_grad():\n        for inputs in tqdm(test_loader, desc=\"Scoring sentences\"):\n            inputs = inputs.to(device)\n            scores = model(inputs)\n            all_scores.append(scores.cpu().numpy())\n    \n    # Combine all batch results\n    sentence_scores = np.concatenate(all_scores, axis=0)\n    \n    # Step 7: Generate summaries\n    print(\"Extracting top sentences for summaries...\")\n    predicted_summaries = extract_top_sentences(\n        sentence_scores, \n        original_articles, \n        top_k=3\n    )\n    \n    # Step 8: Compute ROUGE scores\n    print(\"Computing ROUGE scores...\")\n    rouge_scores = compute_rouge(predicted_summaries, highlights)\n    \n    # Step 9: Print results\n    print(\"\\nROUGE Scores:\")\n    for key, value in rouge_scores.items():\n        print(f\"{key}: {value:.4f}\")\n    \n    # Step 10: Save predictions\n    results_df = pd.DataFrame({\n        'article': original_articles,\n        'highlights': highlights,\n        'predicted_summary': predicted_summaries\n    })\n    results_df.to_csv(\"test_predictions.csv\", index=False)\n    print(\"Saved predictions to test_predictions.csv\")\n    \n    return rouge_scores\n\n# ===== Step 8: Run Evaluation =====\nif __name__ == \"__main__\":\n    # Get word2vec embeddings onto GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    word2vec_vectors = word2vec_vectors.to(device)\n    \n    # Run evaluation\n    evaluate_model(\n        model_path=\"/kaggle/input/word2vec-model/final_model.pt\",\n        test_data_path=\"/kaggle/input/sampled-20k/test_2000.csv\",\n        word2idx=word2idx,\n        embedding_vectors=word2vec_vectors\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:22:13.966329Z","iopub.execute_input":"2025-04-13T05:22:13.966676Z","iopub.status.idle":"2025-04-13T05:23:14.063975Z","shell.execute_reply.started":"2025-04-13T05:22:13.966654Z","shell.execute_reply":"2025-04-13T05:23:14.063221Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 20k Fasttext","metadata":{}},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"# Function to train model on a single file\ndef train_on_file(model, file_path, device, epochs=20):\n    print(f\"\\nTraining on file: {os.path.basename(file_path)}\")\n    \n    # Load embeddings from file to CPU first\n    embeddings = torch.load(file_path, map_location=\"cpu\")\n    \n    # Create dataset and dataloader\n    dataset = TextSummaryDataset(embeddings)\n    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n    \n    # Set up optimizer\n    optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.99, 0.999))\n    \n    # Training loop for this file\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n        for inputs in progress:\n            inputs = inputs.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            \n            # Compute loss\n            loss = torch.mean((outputs - outputs.mean(dim=1, keepdim=True)) ** 2)\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            progress.set_postfix(loss=running_loss / len(dataloader))\n        \n        print(f\"File: {os.path.basename(file_path)} - Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.4f}\")\n    \n    # Clear memory to avoid OOM errors\n    del embeddings, dataset, dataloader\n    torch.cuda.empty_cache()\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:33:41.220879Z","iopub.execute_input":"2025-04-13T04:33:41.221217Z","iopub.status.idle":"2025-04-13T04:33:41.227507Z","shell.execute_reply.started":"2025-04-13T04:33:41.221189Z","shell.execute_reply":"2025-04-13T04:33:41.226818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import glob\ndef main():\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Get all the training files\n    files_part1 = sorted(glob.glob(\"/kaggle/input/fasttext-train-chunk-pt1/*.pt\"))\n    files_part2 = sorted(glob.glob(\"/kaggle/input/fasttext-train-chunk-pt2/*.pt\"))\n    all_files = files_part1 + files_part2\n    \n    print(f\"Found {len(all_files)} training files\")\n    \n    # Check if there's a checkpoint to continue from\n    checkpoint_path = \"model_checkpoint.pt\"\n    start_file_idx = 0\n    \n    if os.path.exists(checkpoint_path):\n        print(f\"Loading checkpoint from {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path)\n        model = checkpoint['model']\n        start_file_idx = checkpoint['file_idx'] + 1\n        print(f\"Continuing from file index {start_file_idx}\")\n    else:\n        # Initialize model\n        # Load first file just to get embedding_dim\n        sample_data = torch.load(all_files[0], map_location=\"cpu\")\n        embedding_dim = sample_data.shape[-1]  # Should be 300 for Word2Vec\n        del sample_data  # Free memory\n        \n        model = WL_AttenSumm(embedding_dim=embedding_dim)\n        print(f\"Initialized new model with embedding_dim={embedding_dim}\")\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Train on each file sequentially\n    for i, file_path in enumerate(all_files[start_file_idx:], start=start_file_idx):\n        print(f\"\\nProcessing file {i+1}/{len(all_files)}: {os.path.basename(file_path)}\")\n        \n        # Train model on this file\n        model = train_on_file(model, file_path, device, epochs=20)\n        \n        # Save checkpoint after each file\n        checkpoint = {\n            'model': model.cpu(),  # Save model to CPU to avoid CUDA memory issues\n            'file_idx': i\n        }\n        torch.save(checkpoint, checkpoint_path)\n        \n        # Move model back to device for next training\n        model = model.to(device)\n        \n        print(f\"Saved checkpoint after file {i+1}/{len(all_files)}\")\n    \n    # Save final model\n    torch.save(model.cpu(), \"final_model.pt\")\n    print(\"Training completed on all files. Final model saved.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T04:36:58.759772Z","iopub.execute_input":"2025-04-13T04:36:58.760076Z","iopub.status.idle":"2025-04-13T04:45:24.995885Z","shell.execute_reply.started":"2025-04-13T04:36:58.760053Z","shell.execute_reply":"2025-04-13T04:45:24.994978Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Testing","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nimport re\nfrom rouge_score import rouge_scorer\n\n# ===== Step 1: Load Word Embeddings =====\nprint(\"Loading fasttext embeddings...\")\n\n# Load your fasttext embeddings - use the vectors you already loaded\nfasttext_vectors = torch.tensor(fasttext.vectors, dtype=torch.float16)  # CPU first\nfasttext_vocab = fasttext.index_to_key\nword2idx = {word: idx for idx, word in enumerate(fasttext_vocab)}\n\n# ===== Step 2: Text Processing Functions =====\ndef preprocess_text(text):\n    \"\"\"Clean and preprocess text for embedding.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Remove special characters and numbers, keep only alphabets and spaces\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n    \n    # Replace multiple spaces with single space\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text.lower().strip()\n\ndef split_into_sentences(text):\n    \"\"\"Split text into sentences.\"\"\"\n    if not isinstance(text, str):\n        return []\n    \n    # Basic sentence splitting on punctuation followed by space\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    return [s.strip() for s in sentences if s.strip()]\n\n# ===== Step 3: Embedding Generation Functions =====\nMAX_SENTENCES = 50\nMAX_WORDS = 50\nEMBEDDING_DIM = fasttext_vectors.shape[1]  # Use fasttext dimension\n\ndef get_embedding(word, word2idx, embedding_vectors):\n    \"\"\"Retrieve word embedding.\"\"\"\n    idx = word2idx.get(word, None)\n    if idx is not None:\n        return embedding_vectors[idx]\n    \n    # For FastText, we can use its ability to handle OOV words if word not in vocabulary\n    # Check if we have access to the fasttext model and use its get_vector method\n    if hasattr(fasttext, 'get_vector'):\n        try:\n            return torch.tensor(fasttext.get_vector(word), dtype=torch.float16)\n        except:\n            pass\n            \n    return torch.zeros(EMBEDDING_DIM, dtype=torch.float16)  # Zero vector for unknown words\n\ndef sentence_to_vector(sentence, word2idx, embedding_vectors, max_words=MAX_WORDS):\n    \"\"\"Convert a sentence into a word embedding matrix.\"\"\"\n    words = preprocess_text(sentence).split()[:max_words]\n    embedding_matrix = torch.zeros((max_words, EMBEDDING_DIM), dtype=torch.float16)\n\n    for i, word in enumerate(words):\n        embedding_matrix[i] = get_embedding(word, word2idx, embedding_vectors)\n\n    return embedding_matrix\n\ndef article_to_vectors(sentences, word2idx, embedding_vectors, max_sentences=MAX_SENTENCES):\n    \"\"\"Convert all sentences in an article to a padded 3D tensor.\"\"\"\n    sentence_vectors = [\n        sentence_to_vector(sent, word2idx, embedding_vectors) \n        for sent in sentences[:max_sentences]\n    ]\n\n    # Pad if necessary\n    num_sentences = len(sentence_vectors)\n    if num_sentences < max_sentences:\n        padding = [torch.zeros((MAX_WORDS, EMBEDDING_DIM), dtype=torch.float16)] * (max_sentences - num_sentences)\n        return torch.stack(sentence_vectors + padding)\n\n    return torch.stack(sentence_vectors[:max_sentences])\n\n# ===== Step 4: Process Test Dataset =====\ndef process_test_dataset(df_path, word2idx, embedding_vectors, batch_size=100):\n    \"\"\"Process test dataset and convert to embeddings.\"\"\"\n    print(f\"Loading test dataset from {df_path}...\")\n    df = pd.read_csv(df_path)\n    \n    print(f\"Processing {len(df)} test articles...\")\n    all_vectors = []\n    original_articles = []\n    highlights = []\n\n    # Process in batches to manage memory\n    for start_idx in tqdm(range(0, len(df), batch_size), desc=\"Processing test batches\"):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_df = df.iloc[start_idx:end_idx]\n        \n        batch_vectors = []\n        for article in batch_df[\"article\"]:\n            sentences = split_into_sentences(article)\n            article_vectors = article_to_vectors(sentences, word2idx, embedding_vectors)\n            batch_vectors.append(article_vectors)\n            \n        # Store original articles and highlights for later use\n        original_articles.extend(batch_df[\"article\"].tolist())\n        highlights.extend(batch_df[\"highlights\"].tolist())\n        \n        # Stack batch and add to result\n        if batch_vectors:\n            all_vectors.append(torch.stack(batch_vectors))\n    \n    # Combine all batches\n    test_embeddings = torch.cat(all_vectors)\n    print(f\"Generated embeddings shape: {test_embeddings.shape}\")\n    \n    return test_embeddings, original_articles, highlights\n\n# ===== Step 5: TextSummaryDataset Class =====\nclass TextSummaryDataset(Dataset):\n    def __init__(self, embeddings):\n        self.embeddings = embeddings.cpu()  # Ensure embeddings are on CPU\n\n    def __len__(self):\n        return len(self.embeddings)\n\n    def __getitem__(self, idx):\n        x = self.embeddings[idx]  # Shape: (50, 50, 300)\n        x = x.mean(dim=1)  # Mean pooling over words (50, 300)\n        return torch.tensor(x, dtype=torch.float32)\n\n# ===== Step 6: Summary Extraction and Evaluation =====\ndef extract_top_sentences(scores, articles, top_k=3):\n    \"\"\"Extract top-k sentences from each article based on scores.\"\"\"\n    summaries = []\n    \n    for i, article in enumerate(articles):\n        sentences = split_into_sentences(article)\n        \n        if not sentences:\n            summaries.append(\"\")\n            continue\n            \n        # Get actual number of sentences\n        num_sentences = min(len(sentences), scores.shape[1])\n        \n        # Get scores for this article\n        article_scores = scores[i][:num_sentences]\n        \n        # Get indices of top-k sentences\n        if len(article_scores) <= top_k:\n            top_indices = np.arange(len(article_scores))\n        else:\n            top_indices = np.argsort(-article_scores)[:top_k]\n        \n        # Sort indices to maintain original order\n        top_indices = sorted(top_indices)\n        \n        # Extract selected sentences\n        selected_sentences = [sentences[idx] for idx in top_indices if idx < len(sentences)]\n        \n        # Join sentences\n        summary = \" \".join(selected_sentences)\n        summaries.append(summary)\n    \n    return summaries\n\ndef compute_rouge(predicted_summaries, gold_summaries):\n    \"\"\"Compute ROUGE scores.\"\"\"\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n    \n    for pred, gold in zip(predicted_summaries, gold_summaries):\n        # Handle empty strings\n        if not isinstance(pred, str) or not pred.strip():\n            pred = \"empty summary\"\n        if not isinstance(gold, str) or not gold.strip():\n            gold = \"empty summary\"\n            \n        score = scorer.score(gold, pred)\n        \n        for key in scores:\n            scores[key].append(score[key].fmeasure)\n    \n    # Calculate average scores\n    avg_scores = {key: np.mean(val) for key, val in scores.items()}\n    \n    return avg_scores\n\n# ===== Step 7: Main Evaluation Function =====\ndef evaluate_model(model_path, test_data_path, word2idx, embedding_vectors):\n    \"\"\"End-to-end evaluation pipeline.\"\"\"\n    print(f\"Starting evaluation pipeline...\")\n    \n    # Step 1: Load model\n    print(f\"Loading model from {model_path}...\")\n    model = torch.load(model_path)\n    \n    # Step 2: Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    model = model.to(device)\n    \n    # Step 3: Generate test embeddings\n    print(\"Generating test embeddings...\")\n    test_embeddings, original_articles, highlights = process_test_dataset(\n        test_data_path, word2idx, embedding_vectors\n    )\n    \n    # Step 4: Save embeddings to avoid regenerating (optional)\n    torch.save(test_embeddings, \"fasttext_test_embeddings.pt\")\n    print(\"Saved test embeddings to fasttext_test_embeddings.pt\")\n    \n    # Step 5: Create dataset and dataloader\n    test_dataset = TextSummaryDataset(test_embeddings)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n    \n    # Step 6: Extract sentence scores\n    model.eval()\n    all_scores = []\n    \n    print(\"Generating sentence scores...\")\n    with torch.no_grad():\n        for inputs in tqdm(test_loader, desc=\"Scoring sentences\"):\n            inputs = inputs.to(device)\n            scores = model(inputs)\n            all_scores.append(scores.cpu().numpy())\n    \n    # Combine all batch results\n    sentence_scores = np.concatenate(all_scores, axis=0)\n    \n    # Step 7: Generate summaries\n    print(\"Extracting top sentences for summaries...\")\n    predicted_summaries = extract_top_sentences(\n        sentence_scores, \n        original_articles, \n        top_k=3\n    )\n    \n    # Step 8: Compute ROUGE scores\n    print(\"Computing ROUGE scores...\")\n    rouge_scores = compute_rouge(predicted_summaries, highlights)\n    \n    # Step 9: Print results\n    print(\"\\nROUGE Scores:\")\n    for key, value in rouge_scores.items():\n        print(f\"{key}: {value:.4f}\")\n    \n    # Step 10: Save predictions\n    results_df = pd.DataFrame({\n        'article': original_articles,\n        'highlights': highlights,\n        'predicted_summary': predicted_summaries\n    })\n    results_df.to_csv(\"fasttext_test_predictions.csv\", index=False)\n    print(\"Saved predictions to fasttext_test_predictions.csv\")\n    \n    return rouge_scores\n\n# ===== Step 8: Run Evaluation =====\nif __name__ == \"__main__\":\n    # Get fasttext embeddings onto GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    fasttext_vectors = fasttext_vectors.to(device)\n    \n    # Run evaluation using fasttext\n    evaluate_model(\n        model_path=\"/kaggle/input/fasttext-model/final_model(2).pt\",  # You might need to use a different model trained with fasttext\n        test_data_path=\"/kaggle/input/sampled-20k/test_2000.csv\",\n        word2idx=word2idx,\n        embedding_vectors=fasttext_vectors\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:03:16.070866Z","iopub.execute_input":"2025-04-13T05:03:16.071262Z","iopub.status.idle":"2025-04-13T05:04:11.215894Z","shell.execute_reply.started":"2025-04-13T05:03:16.071234Z","shell.execute_reply":"2025-04-13T05:04:11.214899Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 20k Glove","metadata":{}},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom tqdm import tqdm\nimport glob\n\n\ndef train_on_file(model, file_path, device, epochs=20):\n    print(f\"\\nTraining on file: {os.path.basename(file_path)}\")\n    \n    # Load embeddings from file to CPU first\n    embeddings = torch.load(file_path, map_location=\"cpu\")\n    \n    # Create dataset and dataloader\n    dataset = TextSummaryDataset(embeddings)\n    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n    \n    # Set up optimizer\n    optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.99, 0.999))\n    \n    # Training loop for this file\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n        for inputs in progress:\n            inputs = inputs.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            \n            # Compute loss\n            loss = torch.mean((outputs - outputs.mean(dim=1, keepdim=True)) ** 2)\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            progress.set_postfix(loss=running_loss / len(dataloader))\n        \n        print(f\"File: {os.path.basename(file_path)} - Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.4f}\")\n    \n    # Clear memory to avoid OOM errors\n    del embeddings, dataset, dataloader\n    torch.cuda.empty_cache()\n    \n    return model\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:04:45.955393Z","iopub.execute_input":"2025-04-13T05:04:45.955750Z","iopub.status.idle":"2025-04-13T05:04:45.964843Z","shell.execute_reply.started":"2025-04-13T05:04:45.955722Z","shell.execute_reply":"2025-04-13T05:04:45.964187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Get all the training files - modify these paths to point to your GloVe embeddings\n    files_part1 = sorted(glob.glob(\"/kaggle/input/glove-train-chunk-pt1/*.pt\"))\n    files_part2 = sorted(glob.glob(\"/kaggle/input/glove-train-chunk-pt2/*.pt\"))\n    glove_files = files_part1 + files_part2\n\n    \n    print(f\"Found {len(glove_files)} training files\")\n    \n    # Check if there's a checkpoint to continue from\n    checkpoint_path = \"glove_model_checkpoint.pt\"\n    start_file_idx = 0\n    \n    if os.path.exists(checkpoint_path):\n        print(f\"Loading checkpoint from {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path)\n        model = checkpoint['model']\n        start_file_idx = checkpoint['file_idx'] + 1\n        print(f\"Continuing from file index {start_file_idx}\")\n    else:\n        # Initialize model\n        # Load first file just to get embedding_dim\n        sample_data = torch.load(glove_files[0], map_location=\"cpu\")\n        embedding_dim = sample_data.shape[-1]  # Should be 100 for GloVe\n        del sample_data  # Free memory\n        \n        model = WL_AttenSumm(embedding_dim=embedding_dim)\n        print(f\"Initialized new model with embedding_dim={embedding_dim}\")\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Train on each file sequentially\n    for i, file_path in enumerate(glove_files[start_file_idx:], start=start_file_idx):\n        print(f\"\\nProcessing file {i+1}/{len(glove_files)}: {os.path.basename(file_path)}\")\n        \n        # Train model on this file\n        model = train_on_file(model, file_path, device, epochs=20)\n        \n        # Save checkpoint after each file\n        checkpoint = {\n            'model': model.cpu(),  # Save model to CPU to avoid CUDA memory issues\n            'file_idx': i\n        }\n        torch.save(checkpoint, checkpoint_path)\n        \n        # Move model back to device for next training\n        model = model.to(device)\n        \n        print(f\"Saved checkpoint after file {i+1}/{len(glove_files)}\")\n    \n    # Save final model\n    torch.save(model.cpu(), \"glove_final_model.pt\")\n    print(\"Training completed on all files. Final model saved.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:06:07.630317Z","iopub.execute_input":"2025-04-13T05:06:07.630649Z","iopub.status.idle":"2025-04-13T05:10:37.171953Z","shell.execute_reply.started":"2025-04-13T05:06:07.630625Z","shell.execute_reply":"2025-04-13T05:10:37.171018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Testing","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nimport re\nfrom rouge_score import rouge_scorer\n\n# ===== Step 1: Load GloVe Embeddings =====\nprint(\"Loading GloVe embeddings...\")\n\n# Convert the glove_embeddings dictionary to a format similar to word2vec/fasttext\nglove_vocab = list(glove_embeddings.keys())\nglove_vectors = torch.tensor([glove_embeddings[word] for word in glove_vocab], dtype=torch.float16)\nword2idx = {word: idx for idx, word in enumerate(glove_vocab)}\n\n# ===== Step 2: Text Processing Functions =====\ndef preprocess_text(text):\n    \"\"\"Clean and preprocess text for embedding.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Remove special characters and numbers, keep only alphabets and spaces\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n    \n    # Replace multiple spaces with single space\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text.lower().strip()\n\ndef split_into_sentences(text):\n    \"\"\"Split text into sentences.\"\"\"\n    if not isinstance(text, str):\n        return []\n    \n    # Basic sentence splitting on punctuation followed by space\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    return [s.strip() for s in sentences if s.strip()]\n\n# ===== Step 3: Embedding Generation Functions =====\nMAX_SENTENCES = 50\nMAX_WORDS = 50\nEMBEDDING_DIM = glove_vectors.shape[1]  # Use GloVe dimension (should be 100)\n\ndef get_embedding(word, word2idx, embedding_vectors):\n    \"\"\"Retrieve word embedding.\"\"\"\n    idx = word2idx.get(word, None)\n    if idx is not None:\n        return embedding_vectors[idx]\n    \n    # If we have FAISS index for GloVe, we can use it for OOV words\n    # This assumes you have created a FAISS index for GloVe\n    if 'glove_index' in globals() and 'glove_words' in globals():\n        try:\n            word_vector = np.zeros((1, EMBEDDING_DIM), dtype=np.float32)\n            _, nearest = glove_index.search(word_vector, 1)\n            nearest_word = glove_words[nearest[0][0]]\n            return torch.tensor(glove_embeddings[nearest_word], dtype=torch.float16)\n        except:\n            pass\n            \n    return torch.zeros(EMBEDDING_DIM, dtype=torch.float16)  # Zero vector for unknown words\n\ndef sentence_to_vector(sentence, word2idx, embedding_vectors, max_words=MAX_WORDS):\n    \"\"\"Convert a sentence into a word embedding matrix.\"\"\"\n    words = preprocess_text(sentence).split()[:max_words]\n    embedding_matrix = torch.zeros((max_words, EMBEDDING_DIM), dtype=torch.float16)\n\n    for i, word in enumerate(words):\n        embedding_matrix[i] = get_embedding(word, word2idx, embedding_vectors)\n\n    return embedding_matrix\n\ndef article_to_vectors(sentences, word2idx, embedding_vectors, max_sentences=MAX_SENTENCES):\n    \"\"\"Convert all sentences in an article to a padded 3D tensor.\"\"\"\n    sentence_vectors = [\n        sentence_to_vector(sent, word2idx, embedding_vectors) \n        for sent in sentences[:max_sentences]\n    ]\n\n    # Pad if necessary\n    num_sentences = len(sentence_vectors)\n    if num_sentences < max_sentences:\n        padding = [torch.zeros((MAX_WORDS, EMBEDDING_DIM), dtype=torch.float16)] * (max_sentences - num_sentences)\n        return torch.stack(sentence_vectors + padding)\n\n    return torch.stack(sentence_vectors[:max_sentences])\n\n# ===== Step 4: Process Test Dataset =====\ndef process_test_dataset(df_path, word2idx, embedding_vectors, batch_size=100):\n    \"\"\"Process test dataset and convert to embeddings.\"\"\"\n    print(f\"Loading test dataset from {df_path}...\")\n    df = pd.read_csv(df_path)\n    \n    print(f\"Processing {len(df)} test articles...\")\n    all_vectors = []\n    original_articles = []\n    highlights = []\n\n    # Process in batches to manage memory\n    for start_idx in tqdm(range(0, len(df), batch_size), desc=\"Processing test batches\"):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_df = df.iloc[start_idx:end_idx]\n        \n        batch_vectors = []\n        for article in batch_df[\"article\"]:\n            sentences = split_into_sentences(article)\n            article_vectors = article_to_vectors(sentences, word2idx, embedding_vectors)\n            batch_vectors.append(article_vectors)\n            \n        # Store original articles and highlights for later use\n        original_articles.extend(batch_df[\"article\"].tolist())\n        highlights.extend(batch_df[\"highlights\"].tolist())\n        \n        # Stack batch and add to result\n        if batch_vectors:\n            all_vectors.append(torch.stack(batch_vectors))\n    \n    # Combine all batches\n    test_embeddings = torch.cat(all_vectors)\n    print(f\"Generated embeddings shape: {test_embeddings.shape}\")\n    \n    return test_embeddings, original_articles, highlights\n\n# ===== Step 5: TextSummaryDataset Class =====\nclass TextSummaryDataset(Dataset):\n    def __init__(self, embeddings):\n        self.embeddings = embeddings.cpu()  # Ensure embeddings are on CPU\n\n    def __len__(self):\n        return len(self.embeddings)\n\n    def __getitem__(self, idx):\n        x = self.embeddings[idx]  # Shape: (50, 50, embedding_dim)\n        x = x.mean(dim=1)  # Mean pooling over words (50, embedding_dim)\n        return torch.tensor(x, dtype=torch.float32)\n\n# ===== Step 6: Summary Extraction and Evaluation =====\ndef extract_top_sentences(scores, articles, top_k=3):\n    \"\"\"Extract top-k sentences from each article based on scores.\"\"\"\n    summaries = []\n    \n    for i, article in enumerate(articles):\n        sentences = split_into_sentences(article)\n        \n        if not sentences:\n            summaries.append(\"\")\n            continue\n            \n        # Get actual number of sentences\n        num_sentences = min(len(sentences), scores.shape[1])\n        \n        # Get scores for this article\n        article_scores = scores[i][:num_sentences]\n        \n        # Get indices of top-k sentences\n        if len(article_scores) <= top_k:\n            top_indices = np.arange(len(article_scores))\n        else:\n            top_indices = np.argsort(-article_scores)[:top_k]\n        \n        # Sort indices to maintain original order\n        top_indices = sorted(top_indices)\n        \n        # Extract selected sentences\n        selected_sentences = [sentences[idx] for idx in top_indices if idx < len(sentences)]\n        \n        # Join sentences\n        summary = \" \".join(selected_sentences)\n        summaries.append(summary)\n    \n    return summaries\n\ndef compute_rouge(predicted_summaries, gold_summaries):\n    \"\"\"Compute ROUGE scores.\"\"\"\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n    \n    for pred, gold in zip(predicted_summaries, gold_summaries):\n        # Handle empty strings\n        if not isinstance(pred, str) or not pred.strip():\n            pred = \"empty summary\"\n        if not isinstance(gold, str) or not gold.strip():\n            gold = \"empty summary\"\n            \n        score = scorer.score(gold, pred)\n        \n        for key in scores:\n            scores[key].append(score[key].fmeasure)\n    \n    # Calculate average scores\n    avg_scores = {key: np.mean(val) for key, val in scores.items()}\n    \n    return avg_scores\n\n# ===== Step 7: Main Evaluation Function =====\ndef evaluate_model(model_path, test_data_path, word2idx, embedding_vectors):\n    \"\"\"End-to-end evaluation pipeline.\"\"\"\n    print(f\"Starting evaluation pipeline...\")\n    \n    # Step 1: Load model\n    print(f\"Loading model from {model_path}...\")\n    model = torch.load(model_path)\n    \n    # Step 2: Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    model = model.to(device)\n    \n    # Step 3: Generate test embeddings\n    print(\"Generating test embeddings...\")\n    test_embeddings, original_articles, highlights = process_test_dataset(\n        test_data_path, word2idx, embedding_vectors\n    )\n    \n    # Step 4: Save embeddings to avoid regenerating (optional)\n    torch.save(test_embeddings, \"glove_test_embeddings.pt\")\n    print(\"Saved test embeddings to glove_test_embeddings.pt\")\n    \n    # Step 5: Create dataset and dataloader\n    test_dataset = TextSummaryDataset(test_embeddings)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n    \n    # Step 6: Extract sentence scores\n    model.eval()\n    all_scores = []\n    \n    print(\"Generating sentence scores...\")\n    with torch.no_grad():\n        for inputs in tqdm(test_loader, desc=\"Scoring sentences\"):\n            inputs = inputs.to(device)\n            scores = model(inputs)\n            all_scores.append(scores.cpu().numpy())\n    \n    # Combine all batch results\n    sentence_scores = np.concatenate(all_scores, axis=0)\n    \n    # Step 7: Generate summaries\n    print(\"Extracting top sentences for summaries...\")\n    predicted_summaries = extract_top_sentences(\n        sentence_scores, \n        original_articles, \n        top_k=3\n    )\n    \n    # Step 8: Compute ROUGE scores\n    print(\"Computing ROUGE scores...\")\n    rouge_scores = compute_rouge(predicted_summaries, highlights)\n    \n    # Step 9: Print results\n    print(\"\\nROUGE Scores:\")\n    for key, value in rouge_scores.items():\n        print(f\"{key}: {value:.4f}\")\n    \n    # Step 10: Save predictions\n    results_df = pd.DataFrame({\n        'article': original_articles,\n        'highlights': highlights,\n        'predicted_summary': predicted_summaries\n    })\n    results_df.to_csv(\"glove_test_predictions.csv\", index=False)\n    print(\"Saved predictions to glove_test_predictions.csv\")\n    \n    return rouge_scores\n\n# ===== Step 8: Run Evaluation =====\nif __name__ == \"__main__\":\n    # Create FAISS index for GloVe if needed\n    if 'glove_index' not in globals() and 'faiss' in globals():\n        print(\"Creating FAISS index for GloVe embeddings...\")\n        # Convert glove embeddings to numpy array\n        glove_words = list(glove_embeddings.keys())\n        glove_vecs = np.array([glove_embeddings[word] for word in tqdm(glove_words, desc=\"Indexing GloVe\")], dtype=np.float32)\n        \n        # Create index\n        glove_index = faiss.IndexFlatL2(EMBEDDING_DIM)\n        glove_index.add(glove_vecs)\n        print(\"FAISS index created for GloVe embeddings\")\n    \n    # Get GloVe embeddings onto GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    glove_vectors = glove_vectors.to(device)\n    \n    # Run evaluation using GloVe\n    evaluate_model(\n        model_path=\"/kaggle/input/glove-model-final/glove_final_model.pt\",  # Use a model trained with GloVe\n        test_data_path=\"/kaggle/input/sampled-20k/test_2000.csv\",\n        word2idx=word2idx,\n        embedding_vectors=glove_vectors\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:13:25.736891Z","iopub.execute_input":"2025-04-13T05:13:25.737307Z","iopub.status.idle":"2025-04-13T05:20:16.351863Z","shell.execute_reply.started":"2025-04-13T05:13:25.737280Z","shell.execute_reply":"2025-04-13T05:20:16.350851Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualisation","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n# Data\ndata = {\n    \"Dataset\": [\"1k\", \"1k\", \"1k\", \"20k\", \"20k\", \"20k\", \"200k\", \"200k\", \"200k\"],\n    \"Embedding\": [\"GloVe\", \"FastText\", \"Word2Vec\"] * 3,\n    \"ROUGE-1\": [0.1911, 0.2151, 0.2307, 0.3724, 0.3208, 0.3567, 42.9, 42.3, 41.8],\n    \"ROUGE-2\": [0.0662, 0.0730, 0.0828, 0.1541, 0.1148, 0.1388, 19.7, 19.2, 18.9],\n    \"ROUGE-L\": [0.1478, 0.1576, 0.1692, 0.2335, 0.1994, 0.2212, 39.3, 38.9, 38.5]\n}\n\ndf = pd.DataFrame(data)\n\n# Normalize paper scores (optional toggle)\nnormalize = True\nif normalize:\n    df.loc[df['Dataset'] == '200k', ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']] = df.loc[df['Dataset'] == '200k', ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']] / 100\n\n# ---- 1. Bar Plots ----\nfor rouge_type in ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']:\n    plt.figure(figsize=(8, 5))\n    sns.barplot(data=df, x='Dataset', y=rouge_type, hue='Embedding')\n    plt.title(f'{rouge_type} by Embedding Type and Dataset Size')\n    plt.ylabel(rouge_type)\n    plt.xlabel(\"Dataset Size\")\n    plt.ylim(0, 0.5)\n    plt.legend(title='Embedding')\n    plt.tight_layout()\n    plt.show()\n\n# ---- 2. Line Plot: Score vs Dataset size ----\ndataset_map = {'1k': 1_000, '20k': 20_000, '200k': 200_000}\ndf['Dataset Size'] = df['Dataset'].map(dataset_map)\n\nfor rouge_type in ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']:\n    plt.figure(figsize=(8, 5))\n    for embedding in df['Embedding'].unique():\n        sub_df = df[df['Embedding'] == embedding]\n        plt.plot(sub_df['Dataset Size'], sub_df[rouge_type], marker='o', label=embedding)\n    plt.title(f'{rouge_type} vs Dataset Size')\n    plt.xlabel('Dataset Size')\n    plt.ylabel(rouge_type)\n    plt.xscale('log')\n    plt.ylim(0, 0.5)\n    plt.legend()\n    plt.grid(True, which=\"both\", ls=\"--\")\n    plt.tight_layout()\n    plt.show()\n\n# ---- 3. Heatmap ----\nheatmap_df = df.pivot(index=\"Embedding\", columns=\"Dataset\", values=\"ROUGE-1\")\nplt.figure(figsize=(6, 4))\nsns.heatmap(heatmap_df, annot=True, cmap='YlGnBu')\nplt.title(\"Heatmap: ROUGE-1\")\nplt.show()\n\nheatmap_df = df.pivot(index=\"Embedding\", columns=\"Dataset\", values=\"ROUGE-2\")\nplt.figure(figsize=(6, 4))\nsns.heatmap(heatmap_df, annot=True, cmap='YlGnBu')\nplt.title(\"Heatmap: ROUGE-2\")\nplt.show()\n\nheatmap_df = df.pivot(index=\"Embedding\", columns=\"Dataset\", values=\"ROUGE-L\")\nplt.figure(figsize=(6, 4))\nsns.heatmap(heatmap_df, annot=True, cmap='YlGnBu')\nplt.title(\"Heatmap: ROUGE-L\")\nplt.show()\n\n# ---- 4. Radar Plot ----\n# def radar_plot(df, dataset_label):\n#     labels = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n#     angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n#     angles += angles[:1]\n\n#     fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n    \n#     for embedding in df['Embedding'].unique():\n#         values = df[(df['Dataset'] == dataset_label) & (df['Embedding'] == embedding)][labels].values.flatten().tolist()\n#         values += values[:1]\n#         ax.plot(angles, values, label=embedding)\n#         ax.fill(angles, values, alpha=0.1)\n    \n#     ax.set_title(f'Radar Plot for {dataset_label} Dataset')\n#     ax.set_xticks(angles[:-1])\n#     ax.set_xticklabels(labels)\n#     ax.set_yticklabels([])\n#     ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n#     plt.tight_layout()\n#     plt.show()\n\n# for dataset in df['Dataset'].unique():\n#     radar_plot(df, dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:51:40.926105Z","iopub.execute_input":"2025-04-13T05:51:40.926445Z","iopub.status.idle":"2025-04-13T05:51:43.310043Z","shell.execute_reply.started":"2025-04-13T05:51:40.926419Z","shell.execute_reply":"2025-04-13T05:51:43.308943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\noutput_dir = \"/kaggle/working/\"\nfor f in os.listdir(output_dir):\n    os.remove(os.path.join(output_dir, f))\nprint(\"🧹 Cleared files from working directory.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 30k dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nimport gensim\nimport swifter\nimport faiss\nimport pickle\nfrom tqdm.auto import tqdm\nfrom nltk.tokenize import sent_tokenize\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import KeyedVectors\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Enable tqdm for Pandas\ntqdm.pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:14:10.543982Z","iopub.execute_input":"2025-04-18T09:14:10.544424Z","iopub.status.idle":"2025-04-18T09:14:10.550142Z","shell.execute_reply.started":"2025-04-18T09:14:10.544394Z","shell.execute_reply":"2025-04-18T09:14:10.549209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load only the last N rows (let's assume we read last 50,000 rows for randomness at the end)\ndf = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv', skiprows=range(1, 100000))  # adjust based on dataset size\n\n# Sample 10,000 random rows from the loaded subset\nsampled_df = df.sample(n=10000, random_state=42)\n\n# Save to a new CSV\nsampled_df.to_csv('/kaggle/working/train_sampled_10k.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:22:31.843357Z","iopub.execute_input":"2025-04-18T09:22:31.843732Z","iopub.status.idle":"2025-04-18T09:22:56.248104Z","shell.execute_reply.started":"2025-04-18T09:22:31.843702Z","shell.execute_reply":"2025-04-18T09:22:56.247157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 1: Load Dataset ===\nprint(\"📥 Loading dataset...\")\ntrain_df = pd.read_csv('/kaggle/input/sampled-10k/train_sampled_10k.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:26:49.154149Z","iopub.execute_input":"2025-04-18T09:26:49.154505Z","iopub.status.idle":"2025-04-18T09:26:50.341788Z","shell.execute_reply.started":"2025-04-18T09:26:49.154482Z","shell.execute_reply":"2025-04-18T09:26:50.341000Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 2: Fast Sentence Tokenization Using Swifter + Progress Bar ===\nprint(\"✂️ Fast tokenizing sentences with Swifter & progress tracking...\")\ntrain_df[\"sentences\"] = train_df[\"article\"].astype(str).swifter.apply(sent_tokenize)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:27:08.263502Z","iopub.execute_input":"2025-04-18T09:27:08.263792Z","iopub.status.idle":"2025-04-18T09:27:20.192827Z","shell.execute_reply.started":"2025-04-18T09:27:08.263770Z","shell.execute_reply":"2025-04-18T09:27:20.191905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Save tokenized sentences\ntrain_df.to_csv(\"/kaggle/working/train_tokenized_10k.csv\", index=False)\nprint(\"✅ Tokenized sentences saved!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:28:03.076253Z","iopub.execute_input":"2025-04-18T09:28:03.076615Z","iopub.status.idle":"2025-04-18T09:28:05.076858Z","shell.execute_reply.started":"2025-04-18T09:28:03.076589Z","shell.execute_reply":"2025-04-18T09:28:05.076112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_and_save(df, prefix, chunk_size=1000):\n    \"\"\"Splits a dataframe into chunks and saves each as a separate file.\"\"\"\n    total_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size != 0 else 0)\n\n    for i in range(total_chunks):\n        start = i * chunk_size\n        end = min((i + 1) * chunk_size, len(df))\n        chunk_df = df.iloc[start:end]\n        chunk_df.to_csv(f\"{prefix}_chunk_{i+1}.csv\", index=False)\n        print(f\"✅ Saved {prefix}_chunk_{i+1}.csv\")\n\n# Split & save training, validation, and test sets\nsplit_and_save(train_df, \"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:30:57.799703Z","iopub.execute_input":"2025-04-18T09:30:57.800049Z","iopub.status.idle":"2025-04-18T09:30:59.769201Z","shell.execute_reply.started":"2025-04-18T09:30:57.800023Z","shell.execute_reply":"2025-04-18T09:30:59.768440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom tqdm.auto import tqdm\n\n# === Step 1: Check & Set Device ===\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"🚀 Using device: {device}\")\n\n# === Step 2: Move Word2Vec Model to GPU ===\nword2vec_vectors = torch.tensor(word2vec.vectors, dtype=torch.float16, device=device)  # Move embeddings to GPU\nword2vec_vocab = word2vec.index_to_key\nword2idx = {word: idx for idx, word in enumerate(word2vec_vocab)}\n\n# === Step 3: Move GloVe & FastText to GPU ===\nglove_vocab = list(glove_embeddings.keys())\nglove_vectors = torch.tensor([glove_embeddings[word] for word in glove_vocab], dtype=torch.float16, device=device)\nglove_word2idx = {word: idx for idx, word in enumerate(glove_vocab)}\n\nfasttext_vocab = fasttext.index_to_key\nfasttext_vectors = torch.tensor(fasttext.vectors, dtype=torch.float16, device=device)\nfasttext_word2idx = {word: idx for idx, word in enumerate(fasttext_vocab)}\n\n# === Step 4: Optimized Word Embedding Lookup on GPU ===\ndef get_embedding(word, word2idx, embedding_vectors):\n    \"\"\"Retrieve word embedding from GPU tensors.\"\"\"\n    idx = word2idx.get(word, None)\n    if idx is not None:\n        return embedding_vectors[idx]  # GPU lookup\n    return torch.zeros_like(embedding_vectors[0])  # Zero vector for unknown words\n\n# === Step 5: Convert Sentences to Word Embeddings Using Mini-Batches ===\nMAX_SENTENCES = 50\nMAX_WORDS = 50\nEMBEDDING_DIM_W2V = 300\nEMBEDDING_DIM_GLOVE = 100\nEMBEDDING_DIM_FASTTEXT = 300\nBATCH_SIZE = 500  # 🚀 Process in batches to avoid OOM\n\ndef sentence_to_vector(sentence, word2idx, embedding_vectors, embedding_dim, max_words=MAX_WORDS):\n    \"\"\"Convert a sentence into a GPU-accelerated word embedding matrix.\"\"\"\n    words = sentence.split()[:max_words]\n    embedding_matrix = torch.zeros((max_words, embedding_dim), dtype=torch.float16, device=device)\n\n    for i, word in enumerate(words):\n        embedding_matrix[i] = get_embedding(word, word2idx, embedding_vectors)\n\n    return embedding_matrix\n\ndef article_to_vectors(sentences, word2idx, embedding_vectors, embedding_dim, max_sentences=MAX_SENTENCES):\n    \"\"\"Convert all sentences in an article to a padded 3D tensor.\"\"\"\n    sentence_vectors = [sentence_to_vector(sent, word2idx, embedding_vectors, embedding_dim) for sent in sentences]\n\n    # Pad or truncate to MAX_SENTENCES\n    num_sentences = len(sentence_vectors)\n    if num_sentences < max_sentences:\n        padding = [torch.zeros((MAX_WORDS, embedding_dim), dtype=torch.float16, device=device)] * (max_sentences - num_sentences)\n        return torch.stack(sentence_vectors + padding)\n\n    return torch.stack(sentence_vectors[:max_sentences])\n\n# === Step 6: Process in Mini-Batches ===\ndef process_in_batches(df, word2idx, embedding_vectors, embedding_dim, batch_size=BATCH_SIZE):\n    \"\"\"Process dataset in small batches to prevent GPU memory overflow.\"\"\"\n    total_samples = len(df)\n    all_vectors = []\n\n    for start in tqdm(range(0, total_samples, batch_size), desc=\"🚀 Processing batches on GPU\"):\n        end = min(start + batch_size, total_samples)\n        batch_df = df.iloc[start:end]  # Select batch\n\n        batch_vectors = [\n            article_to_vectors(sentences, word2idx, embedding_vectors, embedding_dim)\n            for sentences in batch_df[\"sentences\"]\n        ]\n\n        batch_vectors = torch.stack(batch_vectors).cpu()  # Move to CPU to free GPU memory\n        all_vectors.append(batch_vectors)\n        \n        torch.cuda.empty_cache()  # 🚀 Free GPU memory after each batch\n\n    return torch.cat(all_vectors)  # Combine all batches\n\n\n# === Step 7: Process All Files in Directory & Save Output ===\ninput_dir = \"/kaggle/input/train-chunk-pt2\"\noutput_dir = \"/kaggle/working\"\n\nskip_files = {}\n\nprocessed_files = {f.replace('.pt', '') for f in os.listdir(output_dir) if f.endswith(\".pt\")}\n\nfor file_name in os.listdir(input_dir):\n    if file_name.endswith(\".csv\"):\n        base_name = file_name.replace('.csv', '')\n\n        # Skip if already processed OR explicitly listed in skip_files\n        if base_name in processed_files or base_name in skip_files:\n            print(f\"⏩ Skipping: {file_name}\")\n            continue\n\n        file_path = os.path.join(input_dir, file_name)\n        df = pd.read_csv(file_path)\n\n        print(f\"📂 Processing {file_name} ...\")\n        processed_data = process_in_batches(df, word2idx, word2vec_vectors, EMBEDDING_DIM_W2V)\n\n        # Save output\n        output_file = os.path.join(output_dir, f\"{base_name}.pt\")\n        torch.save(processed_data, output_file)\n        print(f\"✅ Saved {output_file}\")\n\nprint(f\"🎉 Remaining files processed and saved in {output_dir}!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:34:03.780594Z","iopub.execute_input":"2025-04-18T09:34:03.780949Z","iopub.status.idle":"2025-04-18T09:55:08.178929Z","shell.execute_reply.started":"2025-04-18T09:34:03.780926Z","shell.execute_reply":"2025-04-18T09:55:08.176605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# === Step 0: Remove Existing .pt Files from Output Directory ===\nfor f in os.listdir(output_dir):\n    if f.endswith(\".pt\"):\n        os.remove(os.path.join(output_dir, f))\nprint(\"🧹 Cleared previous .pt files from working directory.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hierarchial Attention Mechanism","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass WordEncoder(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, num_filters=100, filter_sizes=[1,2,3,4,5,6,7]):\n        super().__init__()\n        self.convs = nn.ModuleList([\n            nn.Conv1d(embedding_dim, num_filters, fs, padding=fs//2)\n            for fs in filter_sizes\n        ])\n        self.gru = nn.GRU(num_filters * len(filter_sizes), hidden_dim, batch_first=True, bidirectional=True)\n        self.attention = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, word_embeddings):\n        # word_embeddings: (batch, max_sentences, max_words, embedding_dim)\n        B, S, W, E = word_embeddings.size()\n        x = word_embeddings.view(B * S, W, E).permute(0, 2, 1)  # (B*S, E, W)\n        convs = [F.relu(conv(x)) for conv in self.convs]\n        pools = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in convs]  # (B*S, num_filters)\n        x_cnn = torch.cat(pools, dim=1).unsqueeze(1).expand(-1, W, -1)  # (B*S, W, CNN_DIM)\n        x_gru, _ = self.gru(x_cnn)\n        attn_weights = torch.softmax(self.attention(x_gru).squeeze(-1), dim=1)\n        sent_vec = torch.sum(attn_weights.unsqueeze(-1) * x_gru, dim=1)  # (B*S, hidden_dim*2)\n        return sent_vec.view(B, S, -1)  # (B, S, hidden_dim*2)\n\nclass SentenceEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.attention = nn.Linear(hidden_dim * 2, 1)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, sentence_vecs):\n        x, _ = self.gru(sentence_vecs)  # (B, S, 2*H)\n        attn_weights = torch.softmax(self.attention(x).squeeze(-1), dim=1)\n        doc_vec = torch.sum(attn_weights.unsqueeze(-1) * x, dim=1)  # (B, 2*H)\n        scores = torch.sigmoid(self.fc(x)).squeeze(-1)  # (B, S)\n        return scores\n\nclass HierarchicalAttentionSummarizer(nn.Module):\n    def __init__(self, embedding_dim, word_hidden_dim=256, sent_hidden_dim=256):\n        super().__init__()\n        self.word_encoder = WordEncoder(embedding_dim, word_hidden_dim)\n        self.sent_encoder = SentenceEncoder(word_hidden_dim * 2, sent_hidden_dim)\n\n    def forward(self, x):\n        # x: (batch_size, max_sentences, max_words, embedding_dim)\n        sent_vecs = self.word_encoder(x)  # (B, S, 2*H)\n        sentence_scores = self.sent_encoder(sent_vecs)  # (B, S)\n        return sentence_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:43:25.509061Z","iopub.execute_input":"2025-04-21T10:43:25.509423Z","iopub.status.idle":"2025-04-21T10:43:25.633777Z","shell.execute_reply.started":"2025-04-21T10:43:25.509384Z","shell.execute_reply":"2025-04-21T10:43:25.632916Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Word2vec","metadata":{}},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nclass TextSummaryDataset(Dataset):\n    def __init__(self, embeddings, dtype=torch.float32):\n        \"\"\"\n        Dataset for text summarization using word embeddings\n        \n        Args:\n            embeddings: Tensor of shape (n_documents, max_sentences, max_words, embedding_dim)\n            dtype: Data type to convert embeddings to\n        \"\"\"\n        # Convert embeddings to specified dtype\n        self.embeddings = embeddings.to(dtype)\n    \n    def __len__(self):\n        return len(self.embeddings)\n    \n    def __getitem__(self, idx):\n        return self.embeddings[idx]\n\ndef train_on_file(model, file_path, device, dtype=torch.float32, epochs=20):\n    print(f\"\\nTraining on file: {os.path.basename(file_path)}\")\n    \n    # Load embeddings from file to CPU first\n    embeddings = torch.load(file_path, map_location=\"cpu\")\n    \n    # Print embeddings data type for debugging\n    print(f\"Original embeddings dtype: {embeddings.dtype}\")\n    \n    # Create dataset and dataloader with explicit dtype conversion\n    dataset = TextSummaryDataset(embeddings, dtype=dtype)\n    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n    \n    # Set up optimizer\n    optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.99, 0.999))\n    \n    # Training loop for this file\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n        for inputs in progress:\n            # Ensure inputs are in the correct dtype before sending to device\n            inputs = inputs.to(dtype).to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass\n            sentence_scores = model(inputs)\n            \n            # Compute variance loss - encouraging diversity in sentence scores\n            loss = torch.mean((sentence_scores - sentence_scores.mean(dim=1, keepdim=True)) ** 2)\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            progress.set_postfix(loss=running_loss / (progress.n + 1))\n        \n        print(f\"File: {os.path.basename(file_path)} - Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.4f}\")\n    \n    # Clear memory to avoid OOM errors\n    del embeddings, dataset, dataloader\n    torch.cuda.empty_cache()\n    \n    return model\n\ndef main():\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Set consistent data type throughout the pipeline\n    dtype = torch.float32  # Use float32 instead of float16\n    \n    # Get all the training files\n    files_part1 = sorted(glob.glob(\"/kaggle/input/word2vec-train-10-chunks/*.pt\"))\n    files_part2 = sorted(glob.glob(\"/kaggle/input/word2vec-train-chunks-10-pt2/*.pt\"))\n    all_files = files_part1 + files_part2\n    \n    print(f\"Found {len(all_files)} training files\")\n    \n    # Check if there's a checkpoint to continue from\n    checkpoint_path = \"model_checkpoint.pt\"\n    start_file_idx = 0\n    \n    if os.path.exists(checkpoint_path):\n        print(f\"Loading checkpoint from {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path)\n        model = checkpoint['model']\n        start_file_idx = checkpoint['file_idx'] + 1\n        print(f\"Continuing from file index {start_file_idx}\")\n        \n        # Ensure model parameters are in the correct dtype\n        for param in model.parameters():\n            param.data = param.data.to(dtype)\n    else:\n        # Initialize model\n        # Load first file just to get embedding_dim\n        sample_data = torch.load(all_files[0], map_location=\"cpu\")\n        embedding_dim = sample_data.shape[-1]  # Should be 300 for Word2Vec\n        print(f\"Sample data dtype: {sample_data.dtype}\")\n        del sample_data  # Free memory\n        \n        # Initialize the HierarchicalAttentionSummarizer model\n        model = HierarchicalAttentionSummarizer(\n            embedding_dim=embedding_dim,\n            word_hidden_dim=256,\n            sent_hidden_dim=256\n        )\n        \n        # Ensure model parameters are in the correct dtype\n        model = model.to(dtype)\n        print(f\"Initialized new hierarchical attention model with embedding_dim={embedding_dim}\")\n        \n        # Debug: print model parameter dtypes\n        for name, param in model.named_parameters():\n            print(f\"Parameter {name} dtype: {param.dtype}\")\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Train on each file sequentially\n    for i, file_path in enumerate(all_files[start_file_idx:], start=start_file_idx):\n        print(f\"\\nProcessing file {i+1}/{len(all_files)}: {os.path.basename(file_path)}\")\n        \n        # Train model on this file with explicit dtype\n        model = train_on_file(model, file_path, device, dtype=dtype, epochs=20)\n        \n        # Save checkpoint after each file\n        checkpoint = {\n            'model': model.cpu(),  # Save model to CPU to avoid CUDA memory issues\n            'file_idx': i\n        }\n        torch.save(checkpoint, checkpoint_path)\n        \n        # Move model back to device for next training\n        model = model.to(device)\n        \n        print(f\"Saved checkpoint after file {i+1}/{len(all_files)}\")\n    \n    # Save final model\n    torch.save(model.cpu(), \"final_model.pt\")\n    print(\"Training completed on all files. Final model saved.\")\n\n# Additional utility function to use the trained model for inference\ndef summarize_document(model, document_embeddings, device, dtype=torch.float32, top_k=3):\n    \"\"\"\n    Summarize a document by selecting the top-k most important sentences\n    \n    Args:\n        model: Trained HierarchicalAttentionSummarizer model\n        document_embeddings: Tensor of shape (1, max_sentences, max_words, embedding_dim)\n        device: Device to run inference on\n        dtype: Data type to convert embeddings to\n        top_k: Number of sentences to select for the summary\n    \n    Returns:\n        indices: Indices of the top-k sentences\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        document_embeddings = document_embeddings.to(dtype).to(device)\n        sentence_scores = model(document_embeddings).squeeze(0)\n        \n        # Get indices of top-k sentences\n        _, indices = torch.topk(sentence_scores, min(top_k, len(sentence_scores)))\n        return indices.cpu().numpy()\n\n# Note: Don't execute main() automatically in Kaggle notebook\n# Instead, call it explicitly in your notebook cell\nmain()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T05:03:26.401005Z","iopub.execute_input":"2025-04-20T05:03:26.401387Z","iopub.status.idle":"2025-04-20T07:05:04.710738Z","shell.execute_reply.started":"2025-04-20T05:03:26.401361Z","shell.execute_reply":"2025-04-20T07:05:04.709888Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Testing","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nimport re\nfrom rouge_score import rouge_scorer\nimport gc  # For garbage collection\n\n# ===== Step 1: Load Word Embeddings =====\nprint(\"Loading word embeddings...\")\n\n# Keep embeddings on CPU initially\nword2vec_vectors = torch.tensor(word2vec.vectors, dtype=torch.float32)\nword2vec_vocab = word2vec.index_to_key\nword2idx = {word: idx for idx, word in enumerate(word2vec_vocab)}\n\n# ===== Step 2: Text Processing Functions =====\ndef preprocess_text(text):\n    \"\"\"Clean and preprocess text for embedding.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Remove special characters and numbers, keep only alphabets and spaces\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n    \n    # Replace multiple spaces with single space\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text.lower().strip()\n\ndef split_into_sentences(text):\n    \"\"\"Split text into sentences.\"\"\"\n    if not isinstance(text, str):\n        return []\n    \n    # Basic sentence splitting on punctuation followed by space\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    return [s.strip() for s in sentences if s.strip()]\n\n# ===== Step 3: Embedding Generation Functions =====\nMAX_SENTENCES = 50\nMAX_WORDS = 50\nEMBEDDING_DIM = 300  # Word2Vec dimension\n\ndef get_embedding(word, word2idx, embedding_vectors):\n    \"\"\"Retrieve word embedding.\"\"\"\n    idx = word2idx.get(word, None)\n    if idx is not None:\n        return embedding_vectors[idx].clone()  # Clone to avoid memory issues\n    return torch.zeros(embedding_vectors.size(1), dtype=embedding_vectors.dtype)\n\ndef sentence_to_vector(sentence, word2idx, embedding_vectors, max_words=MAX_WORDS):\n    \"\"\"Convert a sentence into a word embedding matrix.\"\"\"\n    words = preprocess_text(sentence).split()[:max_words]\n    embedding_matrix = torch.zeros((max_words, EMBEDDING_DIM), dtype=torch.float32)\n\n    for i, word in enumerate(words):\n        embedding_matrix[i] = get_embedding(word, word2idx, embedding_vectors)\n\n    return embedding_matrix\n\ndef article_to_vectors(sentences, word2idx, embedding_vectors, max_sentences=MAX_SENTENCES):\n    \"\"\"Convert all sentences in an article to a padded 3D tensor.\"\"\"\n    sentence_vectors = [\n        sentence_to_vector(sent, word2idx, embedding_vectors) \n        for sent in sentences[:max_sentences]\n    ]\n\n    # Pad if necessary\n    num_sentences = len(sentence_vectors)\n    if num_sentences < max_sentences:\n        padding = [torch.zeros((MAX_WORDS, EMBEDDING_DIM), dtype=torch.float32)] * (max_sentences - num_sentences)\n        return torch.stack(sentence_vectors + padding)\n\n    return torch.stack(sentence_vectors[:max_sentences])\n\n# ===== Step 4: Process Test Dataset with Sequential Selection =====\ndef process_test_dataset(df_path, word2idx, embedding_vectors, batch_size=1, num_samples=None):\n    \"\"\"Process test dataset in smaller batches to save memory with optional sequential sampling.\"\"\"\n    print(f\"Loading test dataset from {df_path}...\")\n    \n    # Load only the first num_samples entries if specified\n    if num_samples:\n        print(f\"Will process the first {num_samples} articles from the dataset\")\n        # Read only the first num_samples rows\n        all_df = pd.read_csv(df_path, nrows=num_samples)\n        print(f\"Loaded {len(all_df)} articles\")\n    else:\n        # Load entire CSV\n        all_df = pd.read_csv(df_path)\n    \n    # Process the data\n    all_vectors = []\n    original_articles = []\n    highlights = []\n    sentence_counts = []\n    \n    # Process in small batches\n    for start_idx in tqdm(range(0, len(all_df), batch_size), desc=\"Processing articles\"):\n        end_idx = min(start_idx + batch_size, len(all_df))\n        batch_df = all_df.iloc[start_idx:end_idx]\n        \n        batch_vectors = []\n        batch_sentence_counts = []\n        \n        try:\n            for article in batch_df[\"article\"]:\n                sentences = split_into_sentences(article)\n                article_vectors = article_to_vectors(sentences, word2idx, embedding_vectors)\n                batch_vectors.append(article_vectors)\n                batch_sentence_counts.append(min(len(sentences), MAX_SENTENCES))\n            \n            # Store original articles and highlights\n            original_articles.extend(batch_df[\"article\"].tolist())\n            highlights.extend(batch_df[\"highlights\"].tolist())\n            sentence_counts.extend(batch_sentence_counts)\n            \n            # Stack batch vectors\n            if batch_vectors:\n                batch_tensor = torch.stack(batch_vectors)\n                all_vectors.append(batch_tensor)\n                \n            # Clear memory after each batch\n            del batch_vectors\n            gc.collect()\n            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n            \n        except Exception as e:\n            print(f\"Error processing batch: {e}\")\n            continue\n    \n    print(f\"Total articles processed: {len(original_articles)}\")\n    \n    return all_vectors, original_articles, highlights, sentence_counts\n\n# ===== Step 5: TextSummaryDataset Class =====\nclass TextSummaryDataset(Dataset):\n    def __init__(self, embeddings_list):\n        \"\"\"Dataset that handles batches of embeddings.\n        \n        Args:\n            embeddings_list: List of tensor batches\n        \"\"\"\n        self.embeddings_list = embeddings_list\n        self.total_len = sum(batch.size(0) for batch in embeddings_list)\n        \n        # Calculate cumulative sizes for indexing\n        self.cumulative_sizes = [0]\n        for batch in embeddings_list:\n            self.cumulative_sizes.append(self.cumulative_sizes[-1] + batch.size(0))\n    \n    def __len__(self):\n        return self.total_len\n    \n    def __getitem__(self, idx):\n        # Find which batch contains this index\n        batch_idx = 0\n        while batch_idx < len(self.cumulative_sizes) - 1 and idx >= self.cumulative_sizes[batch_idx + 1]:\n            batch_idx += 1\n        \n        # Get the relative index within the batch\n        rel_idx = idx - self.cumulative_sizes[batch_idx]\n        \n        # Return the item\n        return self.embeddings_list[batch_idx][rel_idx]\n\n# ===== Step 6: Summary Extraction and Evaluation =====\ndef extract_top_sentences(scores, articles, sentence_counts, top_k=3):\n    \"\"\"Extract top-k sentences from each article based on scores.\"\"\"\n    summaries = []\n    \n    for i, article in enumerate(articles):\n        try:\n            sentences = split_into_sentences(article)\n            \n            if not sentences:\n                summaries.append(\"\")\n                continue\n                \n            # Get actual number of sentences\n            num_sentences = sentence_counts[i]\n            \n            # Get scores for this article\n            article_scores = scores[i][:num_sentences]\n            \n            # Get indices of top-k sentences\n            if len(article_scores) <= top_k:\n                top_indices = np.arange(len(article_scores))\n            else:\n                top_indices = np.argsort(-article_scores)[:top_k]\n            \n            # Sort indices to maintain original order\n            top_indices = sorted(top_indices)\n            \n            # Extract selected sentences\n            selected_sentences = [sentences[idx] for idx in top_indices if idx < len(sentences)]\n            \n            # Join sentences\n            summary = \" \".join(selected_sentences)\n            summaries.append(summary)\n        except Exception as e:\n            print(f\"Error extracting summary for article {i}: {e}\")\n            summaries.append(\"\")\n    \n    return summaries\n\ndef compute_rouge(predicted_summaries, gold_summaries):\n    \"\"\"Compute ROUGE scores.\"\"\"\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n    \n    for pred, gold in zip(predicted_summaries, gold_summaries):\n        # Handle empty strings\n        if not isinstance(pred, str) or not pred.strip():\n            pred = \"empty summary\"\n        if not isinstance(gold, str) or not gold.strip():\n            gold = \"empty summary\"\n            \n        score = scorer.score(gold, pred)\n        \n        for key in scores:\n            scores[key].append(score[key].fmeasure)\n    \n    # Calculate average scores\n    avg_scores = {key: np.mean(val) for key, val in scores.items()}\n    \n    return avg_scores\n\n# ===== Step 7: Main Evaluation Function =====\ndef evaluate_model(model_path, test_data_path, word2idx, embedding_vectors, num_samples=None):\n    \"\"\"End-to-end evaluation pipeline with memory optimization and sequential sampling.\"\"\"\n    print(f\"Starting evaluation pipeline with first {num_samples} samples...\")\n    \n    # Step 1: Load model\n    print(f\"Loading model from {model_path}...\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Load on CPU first, then move to GPU if needed\n    checkpoint = torch.load(model_path, map_location='cpu')\n    \n    # Handle both checkpoint format and direct model format\n    if isinstance(checkpoint, dict) and 'model' in checkpoint:\n        model = checkpoint['model']\n    else:\n        model = checkpoint\n    \n    # Move model to device\n    model = model.to(device)\n    print(f\"Using device: {device}\")\n    \n    # Step 2: Generate test embeddings with sequential sampling (first num_samples entries)\n    print(\"Generating test embeddings...\")\n    embedding_batches, original_articles, highlights, sentence_counts = process_test_dataset(\n        test_data_path, word2idx, embedding_vectors, batch_size=1, num_samples=num_samples\n    )\n    \n    # Step 3: Create dataset\n    test_dataset = TextSummaryDataset(embedding_batches)\n    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0)\n    \n    # Step 4: Extract sentence scores\n    model.eval()\n    all_scores = []\n    \n    print(\"Generating sentence scores...\")\n    with torch.no_grad():\n        for batch_idx, inputs in enumerate(tqdm(test_loader, desc=\"Scoring sentences\")):\n            try:\n                # Move batch to device\n                inputs = inputs.to(device)\n                \n                # Forward pass\n                scores = model(inputs)\n                all_scores.append(scores.cpu().numpy())\n                \n                # Clear GPU memory\n                del inputs\n                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n                \n            except Exception as e:\n                print(f\"Error processing batch {batch_idx}: {e}\")\n                # Add empty scores to maintain alignment\n                batch_size = inputs.size(0) if 'inputs' in locals() else 4\n                empty_scores = np.zeros((batch_size, MAX_SENTENCES))\n                all_scores.append(empty_scores)\n                continue\n    \n    # Combine all batch results\n    sentence_scores = np.concatenate(all_scores, axis=0)\n    \n    # Step 5: Generate summaries\n    print(\"Extracting top sentences for summaries...\")\n    predicted_summaries = extract_top_sentences(\n        sentence_scores, \n        original_articles,\n        sentence_counts,\n        top_k=3\n    )\n    \n    # Step 6: Compute ROUGE scores\n    print(\"Computing ROUGE scores...\")\n    rouge_scores = compute_rouge(predicted_summaries, highlights)\n    \n    # Step 7: Print results\n    print(\"\\nROUGE Scores:\")\n    for key, value in rouge_scores.items():\n        print(f\"{key}: {value:.4f}\")\n    \n    # Step 8: Save predictions\n    results_df = pd.DataFrame({\n        'article': original_articles,\n        'highlights': highlights,\n        'predicted_summary': predicted_summaries\n    })\n    \n    # Save in chunks to avoid memory issues\n    chunk_size = 500\n    output_filename = \"hierarchical_test_predictions_sequential.csv\"\n    for i in range(0, len(results_df), chunk_size):\n        chunk = results_df.iloc[i:i+chunk_size]\n        if i == 0:\n            chunk.to_csv(output_filename, index=False)\n        else:\n            chunk.to_csv(output_filename, mode='a', header=False, index=False)\n    \n    print(f\"Saved predictions to {output_filename}\")\n    \n    # Step 9: Save a sample of predictions for manual inspection\n    sample_df = results_df.head(10)\n    sample_filename = \"sample_predictions_sequential.csv\"\n    sample_df.to_csv(sample_filename, index=False)\n    print(f\"Saved sample predictions to {sample_filename}\")\n    \n    return rouge_scores\n\n# ===== Step 8: Run Evaluation =====\nif __name__ == \"__main__\":\n    try:\n        # Keep embeddings on CPU\n        # Only move small batches to GPU as needed during processing\n        print(f\"Word2Vec embeddings shape: {word2vec_vectors.shape}\")\n        \n        # Set number of samples to process (first 1500 entries)\n        num_samples = 1500\n        \n        # Run evaluation with memory optimizations and sequential sampling\n        evaluate_model(\n            model_path=\"/kaggle/input/word2vec-hierarchial-model/final_model.pt\",\n            test_data_path=\"/kaggle/input/sampled-20k/test_2000.csv\",\n            word2idx=word2idx,\n            embedding_vectors=word2vec_vectors,\n            num_samples=num_samples\n        )\n    except Exception as e:\n        print(f\"Evaluation failed: {e}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T04:00:47.250799Z","iopub.execute_input":"2025-04-21T04:00:47.251138Z","iopub.status.idle":"2025-04-21T04:04:20.589139Z","shell.execute_reply.started":"2025-04-21T04:00:47.251112Z","shell.execute_reply":"2025-04-21T04:04:20.588318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Fasttext","metadata":{}},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\n\n# Hierarchical Attention Model definition (copy from your cell)\nclass WordEncoder(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, num_filters=100, filter_sizes=[1,2,3,4,5,6,7]):\n        super().__init__()\n        self.convs = nn.ModuleList([\n            nn.Conv1d(embedding_dim, num_filters, fs, padding=fs//2)\n            for fs in filter_sizes\n        ])\n        self.gru = nn.GRU(num_filters * len(filter_sizes), hidden_dim, batch_first=True, bidirectional=True)\n        self.attention = nn.Linear(hidden_dim * 2, 1)\n        \n    def forward(self, word_embeddings):\n        # word_embeddings: (batch, max_sentences, max_words, embedding_dim)\n        B, S, W, E = word_embeddings.size()\n        x = word_embeddings.view(B * S, W, E).permute(0, 2, 1)  # (B*S, E, W)\n        convs = [F.relu(conv(x)) for conv in self.convs]\n        pools = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in convs]  # (B*S, num_filters)\n        x_cnn = torch.cat(pools, dim=1).unsqueeze(1).expand(-1, W, -1)  # (B*S, W, CNN_DIM)\n        x_gru, _ = self.gru(x_cnn)\n        attn_weights = torch.softmax(self.attention(x_gru).squeeze(-1), dim=1)\n        sent_vec = torch.sum(attn_weights.unsqueeze(-1) * x_gru, dim=1)  # (B*S, hidden_dim*2)\n        return sent_vec.view(B, S, -1), attn_weights.view(B, S, W)  # (B, S, hidden_dim*2), (B, S, W)\n\nclass SentenceEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.attention = nn.Linear(hidden_dim * 2, 1)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n        \n    def forward(self, sentence_vecs):\n        x, _ = self.gru(sentence_vecs)  # (B, S, 2*H)\n        attn_weights = torch.softmax(self.attention(x).squeeze(-1), dim=1)\n        doc_vec = torch.sum(attn_weights.unsqueeze(-1) * x, dim=1)  # (B, 2*H)\n        scores = torch.sigmoid(self.fc(x)).squeeze(-1)  # (B, S)\n        return scores, attn_weights\n\nclass HierarchicalAttentionSummarizer(nn.Module):\n    def __init__(self, embedding_dim, word_hidden_dim=256, sent_hidden_dim=256):\n        super().__init__()\n        self.word_encoder = WordEncoder(embedding_dim, word_hidden_dim)\n        self.sent_encoder = SentenceEncoder(word_hidden_dim * 2, sent_hidden_dim)\n        \n    def forward(self, x):\n        # x: (batch_size, max_sentences, max_words, embedding_dim)\n        sent_vecs, word_attn = self.word_encoder(x)  # (B, S, 2*H), (B, S, W)\n        sentence_scores, sent_attn = self.sent_encoder(sent_vecs)  # (B, S), (B, S)\n        return word_attn, sent_attn, sentence_scores\n\n# Define dataset class for hierarchical input\nclass HierarchicalSummaryDataset(Dataset):\n    def __init__(self, embeddings):\n        self.embeddings = embeddings.cpu()  # Ensure embeddings are on CPU\n\n    def __len__(self):\n        return len(self.embeddings)\n\n    def __getitem__(self, idx):\n        # Return the full 3D tensor (max_sentences, max_words, embedding_dim)\n        return torch.tensor(self.embeddings[idx], dtype=torch.float32)\n\n# Function to train model on a single file\ndef train_on_file(model, file_path, device, epochs=20):\n    print(f\"\\nTraining on file: {os.path.basename(file_path)}\")\n    \n    # Load embeddings from file to CPU first\n    embeddings = torch.load(file_path, map_location=\"cpu\")\n    \n    # Create dataset and dataloader with the hierarchical dataset\n    dataset = HierarchicalSummaryDataset(embeddings)\n    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0)\n    \n    # Set up optimizer\n    optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.99, 0.999))\n    criterion = nn.BCELoss()  # Binary cross-entropy for sentence scoring\n    \n    # Training loop for this file\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n        for inputs in progress:\n            inputs = inputs.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass with hierarchical attention model\n            word_attn, sent_attn, sentence_scores = model(inputs)\n            \n            # Create pseudo-labels (assuming top 3 sentences are summary-worthy)\n            # This is a simplistic approach - in practice, you might have actual labels\n            batch_size, num_sentences = sentence_scores.shape\n            pseudo_labels = torch.zeros_like(sentence_scores)\n            \n            # Set top 3 sentences in each document as positive (1)\n            for i in range(batch_size):\n                # Get scores for this document and find top 3 indices\n                doc_scores = sentence_scores[i]\n                if torch.sum(doc_scores > 0) > 0:  # Check if there are valid scores\n                    _, top_indices = torch.topk(doc_scores, min(3, num_sentences))\n                    pseudo_labels[i, top_indices] = 1.0\n            \n            # Compute loss\n            loss = criterion(sentence_scores, pseudo_labels)\n            \n            # Add regularization for attention distribution if needed\n            # This encourages more diverse attention\n            attn_reg = 0.01 * (torch.mean(torch.abs(word_attn)) + torch.mean(torch.abs(sent_attn)))\n            total_loss = loss + attn_reg\n            \n            # Backward pass\n            total_loss.backward()\n            \n            # Clip gradients to prevent explosion\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            \n            running_loss += total_loss.item()\n            progress.set_postfix(loss=running_loss / (progress.n + 1))\n        \n        print(f\"File: {os.path.basename(file_path)} - Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.4f}\")\n    \n    # Clear memory to avoid OOM errors\n    del embeddings, dataset, dataloader\n    torch.cuda.empty_cache()\n    \n    return model\n\ndef main():\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Get all the training files\n    files_part1 = sorted(glob.glob(\"/kaggle/input/fasttext-train-chunk-pt1/*.pt\"))\n    files_part2 = sorted(glob.glob(\"/kaggle/input/fasttext-train-chunk-pt2/*.pt\"))\n    all_files = files_part1 + files_part2\n    \n    print(f\"Found {len(all_files)} training files\")\n    \n    # Check if there's a checkpoint to continue from\n    checkpoint_path = \"hierarchical_model_checkpoint.pt\"\n    start_file_idx = 0\n    \n    if os.path.exists(checkpoint_path):\n        print(f\"Loading checkpoint from {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path)\n        model = checkpoint['model']\n        start_file_idx = checkpoint['file_idx'] + 1\n        print(f\"Continuing from file index {start_file_idx}\")\n    else:\n        # Initialize model\n        # Load first file just to get embedding dimensions\n        sample_data = torch.load(all_files[0], map_location=\"cpu\")\n        embedding_dim = sample_data.shape[-1]  # Should be 300 for Word2Vec/FastText\n        \n        # Initialize the hierarchical attention model\n        word_hidden_dim = 256\n        sent_hidden_dim = 256\n        \n        model = HierarchicalAttentionSummarizer(\n            embedding_dim=embedding_dim,\n            word_hidden_dim=word_hidden_dim,\n            sent_hidden_dim=sent_hidden_dim\n        )\n        \n        print(f\"Initialized new hierarchical attention model with embedding_dim={embedding_dim}\")\n        del sample_data  # Free memory\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Train on each file sequentially\n    for i, file_path in enumerate(all_files[start_file_idx:], start=start_file_idx):\n        print(f\"\\nProcessing file {i+1}/{len(all_files)}: {os.path.basename(file_path)}\")\n        \n        # Train model on this file\n        model = train_on_file(model, file_path, device, epochs=20)\n        \n        # Save checkpoint after each file\n        checkpoint = {\n            'model': model.cpu(),  # Save model to CPU to avoid CUDA memory issues\n            'file_idx': i\n        }\n        torch.save(checkpoint, checkpoint_path)\n        \n        # Move model back to device for next training\n        model = model.to(device)\n        \n        print(f\"Saved checkpoint after file {i+1}/{len(all_files)}\")\n    \n    # Save final model\n    torch.save(model.cpu(), \"final_hierarchical_model.pt\")\n    print(\"Training completed on all files. Final hierarchical attention model saved.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T04:21:09.778654Z","iopub.execute_input":"2025-04-21T04:21:09.779061Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Testing","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nimport re\nfrom rouge_score import rouge_scorer\n\n# Hierarchical Attention Model definition\nclass WordEncoder(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, num_filters=100, filter_sizes=[1,2,3,4,5,6,7]):\n        super().__init__()\n        self.convs = nn.ModuleList([\n            nn.Conv1d(embedding_dim, num_filters, fs, padding=fs//2)\n            for fs in filter_sizes\n        ])\n        self.gru = nn.GRU(num_filters * len(filter_sizes), hidden_dim, batch_first=True, bidirectional=True)\n        self.attention = nn.Linear(hidden_dim * 2, 1)\n        \n    def forward(self, word_embeddings):\n        # word_embeddings: (batch, max_sentences, max_words, embedding_dim)\n        B, S, W, E = word_embeddings.size()\n        x = word_embeddings.view(B * S, W, E).permute(0, 2, 1)  # (B*S, E, W)\n        convs = [F.relu(conv(x)) for conv in self.convs]\n        pools = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in convs]  # (B*S, num_filters)\n        x_cnn = torch.cat(pools, dim=1).unsqueeze(1).expand(-1, W, -1)  # (B*S, W, CNN_DIM)\n        x_gru, _ = self.gru(x_cnn)\n        attn_weights = torch.softmax(self.attention(x_gru).squeeze(-1), dim=1)\n        sent_vec = torch.sum(attn_weights.unsqueeze(-1) * x_gru, dim=1)  # (B*S, hidden_dim*2)\n        return sent_vec.view(B, S, -1), attn_weights.view(B, S, W)  # (B, S, hidden_dim*2), (B, S, W)\n\nclass SentenceEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.attention = nn.Linear(hidden_dim * 2, 1)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n        \n    def forward(self, sentence_vecs):\n        x, _ = self.gru(sentence_vecs)  # (B, S, 2*H)\n        attn_weights = torch.softmax(self.attention(x).squeeze(-1), dim=1)\n        doc_vec = torch.sum(attn_weights.unsqueeze(-1) * x, dim=1)  # (B, 2*H)\n        scores = torch.sigmoid(self.fc(x)).squeeze(-1)  # (B, S)\n        return scores, attn_weights\n\nclass HierarchicalAttentionSummarizer(nn.Module):\n    def __init__(self, embedding_dim, word_hidden_dim=256, sent_hidden_dim=256):\n        super().__init__()\n        self.word_encoder = WordEncoder(embedding_dim, word_hidden_dim)\n        self.sent_encoder = SentenceEncoder(word_hidden_dim * 2, sent_hidden_dim)\n        \n    def forward(self, x):\n        # x: (batch_size, max_sentences, max_words, embedding_dim)\n        sent_vecs, word_attn = self.word_encoder(x)  # (B, S, 2*H), (B, S, W)\n        sentence_scores, sent_attn = self.sent_encoder(sent_vecs)  # (B, S), (B, S)\n        return word_attn, sent_attn, sentence_scores\n\n# ===== Using already loaded FastText embeddings =====\n# We assume fasttext is already loaded and available as you mentioned\nprint(\"Using already loaded FastText embeddings...\")\nfasttext_vectors = torch.tensor(fasttext.vectors, dtype=torch.float16)\nfasttext_vocab = fasttext.index_to_key\nword2idx = {word: idx for idx, word in enumerate(fasttext_vocab)}\n\n# ===== Text Processing Functions =====\ndef preprocess_text(text):\n    \"\"\"Clean and preprocess text for embedding.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Remove special characters and numbers, keep only alphabets and spaces\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n    \n    # Replace multiple spaces with single space\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text.lower().strip()\n\ndef split_into_sentences(text):\n    \"\"\"Split text into sentences.\"\"\"\n    if not isinstance(text, str):\n        return []\n    \n    # Basic sentence splitting on punctuation followed by space\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    return [s.strip() for s in sentences if s.strip()]\n\n# ===== Embedding Generation Functions =====\nMAX_SENTENCES = 50\nMAX_WORDS = 50\nEMBEDDING_DIM = fasttext_vectors.shape[1]  # Use fasttext dimension\n\ndef get_embedding(word, word2idx, embedding_vectors):\n    \"\"\"Retrieve word embedding.\"\"\"\n    idx = word2idx.get(word, None)\n    if idx is not None:\n        return embedding_vectors[idx]\n    \n    # For FastText, we can use its ability to handle OOV words if word not in vocabulary\n    # Check if we have access to the fasttext model and use its get_vector method\n    if hasattr(fasttext, 'get_vector'):\n        try:\n            return torch.tensor(fasttext.get_vector(word), dtype=torch.float16)\n        except:\n            pass\n            \n    return torch.zeros(EMBEDDING_DIM, dtype=torch.float16)  # Zero vector for unknown words\n\ndef sentence_to_vector(sentence, word2idx, embedding_vectors, max_words=MAX_WORDS):\n    \"\"\"Convert a sentence into a word embedding matrix.\"\"\"\n    words = preprocess_text(sentence).split()[:max_words]\n    embedding_matrix = torch.zeros((max_words, EMBEDDING_DIM), dtype=torch.float16)\n\n    for i, word in enumerate(words):\n        embedding_matrix[i] = get_embedding(word, word2idx, embedding_vectors)\n\n    return embedding_matrix\n\ndef article_to_vectors(sentences, word2idx, embedding_vectors, max_sentences=MAX_SENTENCES):\n    \"\"\"Convert all sentences in an article to a padded 3D tensor.\"\"\"\n    sentence_vectors = [\n        sentence_to_vector(sent, word2idx, embedding_vectors) \n        for sent in sentences[:max_sentences]\n    ]\n\n    # Pad if necessary\n    num_sentences = len(sentence_vectors)\n    if num_sentences < max_sentences:\n        padding = [torch.zeros((MAX_WORDS, EMBEDDING_DIM), dtype=torch.float16)] * (max_sentences - num_sentences)\n        return torch.stack(sentence_vectors + padding)\n\n    return torch.stack(sentence_vectors[:max_sentences])\n\ndef process_test_dataset(df_path, word2idx, embedding_vectors, batch_size=100, max_articles=1500):\n    \"\"\"Process test dataset and convert to embeddings.\"\"\"\n    print(f\"Loading test dataset from {df_path}...\")\n    df = pd.read_csv(df_path)\n    \n    # Limit to the first max_articles (1500)\n    df = df.head(max_articles)\n    \n    print(f\"Processing {len(df)} test articles...\")\n    all_vectors = []\n    original_articles = []\n    highlights = []\n\n    # Process in batches to manage memory\n    for start_idx in tqdm(range(0, len(df), batch_size), desc=\"Processing test batches\"):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_df = df.iloc[start_idx:end_idx]\n        \n        batch_vectors = []\n        for article in batch_df[\"article\"]:\n            sentences = split_into_sentences(article)\n            article_vectors = article_to_vectors(sentences, word2idx, embedding_vectors)\n            batch_vectors.append(article_vectors)\n            \n        # Store original articles and highlights for later use\n        original_articles.extend(batch_df[\"article\"].tolist())\n        highlights.extend(batch_df[\"highlights\"].tolist())\n        \n        # Stack batch and add to result\n        if batch_vectors:\n            all_vectors.append(torch.stack(batch_vectors))\n    \n    # Combine all batches\n    test_embeddings = torch.cat(all_vectors)\n    print(f\"Generated embeddings shape: {test_embeddings.shape}\")\n    \n    return test_embeddings, original_articles, highlights\n\n# ===== HierarchicalSummaryDataset Class =====\nclass HierarchicalSummaryDataset(Dataset):\n    def __init__(self, embeddings):\n        self.embeddings = embeddings.cpu()  # Ensure embeddings are on CPU\n\n    def __len__(self):\n        return len(self.embeddings)\n\n    def __getitem__(self, idx):\n        # Return the full 3D tensor (max_sentences, max_words, embedding_dim)\n        return torch.tensor(self.embeddings[idx], dtype=torch.float32)\n\n# ===== Summary Extraction and Evaluation =====\ndef extract_top_sentences(scores, articles, top_k=3):\n    \"\"\"Extract top-k sentences from each article based on scores.\"\"\"\n    summaries = []\n    \n    for i, article in enumerate(articles):\n        sentences = split_into_sentences(article)\n        \n        if not sentences:\n            summaries.append(\"\")\n            continue\n            \n        # Get actual number of sentences\n        num_sentences = min(len(sentences), scores.shape[1])\n        \n        # Get scores for this article\n        article_scores = scores[i][:num_sentences]\n        \n        # Get indices of top-k sentences\n        if len(article_scores) <= top_k:\n            top_indices = np.arange(len(article_scores))\n        else:\n            top_indices = np.argsort(-article_scores)[:top_k]\n        \n        # Sort indices to maintain original order\n        top_indices = sorted(top_indices)\n        \n        # Extract selected sentences\n        selected_sentences = [sentences[idx] for idx in top_indices if idx < len(sentences)]\n        \n        # Join sentences\n        summary = \" \".join(selected_sentences)\n        summaries.append(summary)\n    \n    return summaries\n\ndef compute_rouge(predicted_summaries, gold_summaries):\n    \"\"\"Compute ROUGE scores.\"\"\"\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n    \n    for pred, gold in zip(predicted_summaries, gold_summaries):\n        # Handle empty strings\n        if not isinstance(pred, str) or not pred.strip():\n            pred = \"empty summary\"\n        if not isinstance(gold, str) or not gold.strip():\n            gold = \"empty summary\"\n            \n        score = scorer.score(gold, pred)\n        \n        for key in scores:\n            scores[key].append(score[key].fmeasure)\n    \n    # Calculate average scores\n    avg_scores = {key: np.mean(val) for key, val in scores.items()}\n    \n    return avg_scores\n\n# ===== Main Evaluation Function =====\ndef evaluate_model(model_path, test_data_path, word2idx, embedding_vectors):\n    \"\"\"End-to-end evaluation pipeline.\"\"\"\n    print(f\"Starting evaluation pipeline...\")\n    \n    # Step 1: Load model\n    print(f\"Loading model from {model_path}...\")\n    model = torch.load(model_path)\n    \n    # Step 2: Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    model = model.to(device)\n    \n    # Step 3: Generate test embeddings\n    print(\"Generating test embeddings...\")\n    test_embeddings, original_articles, highlights = process_test_dataset(\n        test_data_path, word2idx, embedding_vectors, max_articles=1500  # Limit to 1500 articles\n    )\n    \n    # Step 4: Save embeddings to avoid regenerating (optional)\n    torch.save(test_embeddings, \"fasttext_hierarchical_test_embeddings_1500.pt\")\n    print(\"Saved test embeddings to fasttext_hierarchical_test_embeddings_1500.pt\")\n    \n    # Step 5: Create dataset and dataloader for hierarchical model\n    test_dataset = HierarchicalSummaryDataset(test_embeddings)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n    \n    # Step 6: Extract sentence scores\n    model.eval()\n    all_scores = []\n    \n    print(\"Generating sentence scores...\")\n    with torch.no_grad():\n        for inputs in tqdm(test_loader, desc=\"Scoring sentences\"):\n            inputs = inputs.to(device)\n            # The hierarchical model returns word_attn, sent_attn, scores\n            _, _, scores = model(inputs)\n            all_scores.append(scores.cpu().numpy())\n    \n    # Combine all batch results\n    sentence_scores = np.concatenate(all_scores, axis=0)\n    \n    # Step 7: Generate summaries\n    print(\"Extracting top sentences for summaries...\")\n    predicted_summaries = extract_top_sentences(\n        sentence_scores, \n        original_articles, \n        top_k=3\n    )\n    \n    # Step 8: Compute ROUGE scores\n    print(\"Computing ROUGE scores...\")\n    rouge_scores = compute_rouge(predicted_summaries, highlights)\n    \n    # Step 9: Print results\n    print(\"\\nROUGE Scores:\")\n    for key, value in rouge_scores.items():\n        print(f\"{key}: {value:.4f}\")\n    \n    # Step 10: Save predictions\n    results_df = pd.DataFrame({\n        'article': original_articles,\n        'highlights': highlights,\n        'predicted_summary': predicted_summaries\n    })\n    results_df.to_csv(\"hierarchical_test_predictions_1500.csv\", index=False)\n    print(\"Saved predictions to hierarchical_test_predictions_1500.csv\")\n    \n    return rouge_scores\n\n# ===== Run Evaluation =====\nif __name__ == \"__main__\":\n    # Get fasttext embeddings onto GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    fasttext_vectors = fasttext_vectors.to(device)\n    \n    # Run evaluation using fasttext\n    evaluate_model(\n        model_path=\"final_hierarchical_model.pt\",  # Path to your trained hierarchical model\n        test_data_path=\"/kaggle/input/sampled-20k/test_2000.csv\",\n        word2idx=word2idx,\n        embedding_vectors=fasttext_vectors\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Glove","metadata":{}},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom tqdm import tqdm\nimport glob\n\n# Hierarchical Attention Mechanism model definitions\nclass WordEncoder(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, num_filters=100, filter_sizes=[1,2,3,4,5,6,7]):\n        super().__init__()\n        self.convs = nn.ModuleList([\n            nn.Conv1d(embedding_dim, num_filters, fs, padding=fs//2)\n            for fs in filter_sizes\n        ])\n        self.gru = nn.GRU(num_filters * len(filter_sizes), hidden_dim, batch_first=True, bidirectional=True)\n        self.attention = nn.Linear(hidden_dim * 2, 1)\n        \n    def forward(self, word_embeddings):\n        # word_embeddings: (batch, max_sentences, max_words, embedding_dim)\n        B, S, W, E = word_embeddings.size()\n        x = word_embeddings.view(B * S, W, E).permute(0, 2, 1)  # (B*S, E, W)\n        \n        convs = [F.relu(conv(x)) for conv in self.convs]\n        pools = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in convs]  # (B*S, num_filters)\n        x_cnn = torch.cat(pools, dim=1).unsqueeze(1).expand(-1, W, -1)  # (B*S, W, CNN_DIM)\n        \n        x_gru, _ = self.gru(x_cnn)\n        attn_weights = torch.softmax(self.attention(x_gru).squeeze(-1), dim=1)\n        sent_vec = torch.sum(attn_weights.unsqueeze(-1) * x_gru, dim=1)  # (B*S, hidden_dim*2)\n        \n        return sent_vec.view(B, S, -1)  # (B, S, hidden_dim*2)\n\nclass SentenceEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.attention = nn.Linear(hidden_dim * 2, 1)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n        \n    def forward(self, sentence_vecs):\n        x, _ = self.gru(sentence_vecs)  # (B, S, 2*H)\n        attn_weights = torch.softmax(self.attention(x).squeeze(-1), dim=1)\n        doc_vec = torch.sum(attn_weights.unsqueeze(-1) * x, dim=1)  # (B, 2*H)\n        scores = torch.sigmoid(self.fc(x)).squeeze(-1)  # (B, S)\n        \n        return scores\n\nclass HierarchicalAttentionSummarizer(nn.Module):\n    def __init__(self, embedding_dim, word_hidden_dim=256, sent_hidden_dim=256):\n        super().__init__()\n        self.word_encoder = WordEncoder(embedding_dim, word_hidden_dim)\n        self.sent_encoder = SentenceEncoder(word_hidden_dim * 2, sent_hidden_dim)\n        \n    def forward(self, x):\n        # x: (batch_size, max_sentences, max_words, embedding_dim)\n        sent_vecs = self.word_encoder(x)  # (B, S, 2*H)\n        sentence_scores = self.sent_encoder(sent_vecs)  # (B, S)\n        \n        return sentence_scores\n\n# Define the dataset class\nclass TextSummaryDataset(Dataset):\n    def __init__(self, embeddings):\n        self.embeddings = embeddings\n    \n    def __len__(self):\n        return len(self.embeddings)\n    \n    def __getitem__(self, idx):\n        return self.embeddings[idx]\n\ndef train_on_file(model, file_path, device, epochs=20):\n    print(f\"\\nTraining on file: {os.path.basename(file_path)}\")\n    \n    # Load embeddings from file to CPU first\n    embeddings = torch.load(file_path, map_location=\"cpu\")\n    \n    # Convert embeddings to float32 to match model parameters\n    embeddings = embeddings.to(torch.float32)\n    \n    # Create dataset and dataloader\n    dataset = TextSummaryDataset(embeddings)\n    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n    \n    # Set up optimizer\n    optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.99, 0.999))\n    \n    # Training loop for this file\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n        for inputs in progress:\n            inputs = inputs.to(device).to(torch.float32)  # Ensure inputs are float32\n            \n            optimizer.zero_grad()\n            \n            # Forward pass - get sentence scores\n            sentence_scores = model(inputs)\n            \n            # Compute loss - using the same approximation as before\n            # This could be replaced with a more appropriate loss for summarization\n            loss = torch.mean((sentence_scores - sentence_scores.mean(dim=1, keepdim=True)) ** 2)\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            progress.set_postfix(loss=running_loss / len(progress))\n        \n        print(f\"File: {os.path.basename(file_path)} - Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.4f}\")\n    \n    # Clear memory to avoid OOM errors\n    del embeddings, dataset, dataloader\n    torch.cuda.empty_cache()\n    \n    return model\n\ndef main():\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Get all the training files - modify these paths to point to your GloVe embeddings\n    files_part1 = sorted(glob.glob(\"/kaggle/input/glove-train-chunk-pt1/*.pt\"))\n    files_part2 = sorted(glob.glob(\"/kaggle/input/glove-train-chunk-pt2/*.pt\"))\n    glove_files = files_part1 + files_part2\n    \n    print(f\"Found {len(glove_files)} training files\")\n    \n    # Check if there's a checkpoint to continue from\n    checkpoint_path = \"hierarchical_attention_checkpoint.pt\"\n    start_file_idx = 0\n    \n    if os.path.exists(checkpoint_path):\n        print(f\"Loading checkpoint from {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path)\n        model = checkpoint['model']\n        start_file_idx = checkpoint['file_idx'] + 1\n        print(f\"Continuing from file index {start_file_idx}\")\n    else:\n        # Initialize model\n        # Load first file just to get embedding_dim and structure\n        sample_data = torch.load(glove_files[0], map_location=\"cpu\")\n        embedding_dim = sample_data.shape[-1]  # Should be 100 for GloVe\n        \n        # Initialize the hierarchical attention model\n        model = HierarchicalAttentionSummarizer(embedding_dim=embedding_dim)\n        print(f\"Initialized new hierarchical attention model with embedding_dim={embedding_dim}\")\n        \n        del sample_data  # Free memory\n    \n    # Move model to device and ensure it's in float32\n    model = model.to(device).to(torch.float32)\n    \n    # Train on each file sequentially\n    for i, file_path in enumerate(glove_files[start_file_idx:], start=start_file_idx):\n        print(f\"\\nProcessing file {i+1}/{len(glove_files)}: {os.path.basename(file_path)}\")\n        \n        try:\n            # Train model on this file\n            model = train_on_file(model, file_path, device, epochs=20)\n            \n            # Save checkpoint after each file\n            checkpoint = {\n                'model': model.cpu(),  # Save model to CPU to avoid CUDA memory issues\n                'file_idx': i\n            }\n            torch.save(checkpoint, checkpoint_path)\n            \n            # Move model back to device for next training\n            model = model.to(device)\n            \n            print(f\"Saved checkpoint after file {i+1}/{len(glove_files)}\")\n            \n        except Exception as e:\n            print(f\"Error processing file {file_path}: {e}\")\n            # Save current checkpoint before exiting\n            checkpoint = {\n                'model': model.cpu(),\n                'file_idx': i-1  # Record the last successful file\n            }\n            torch.save(checkpoint, checkpoint_path)\n            raise e\n    \n    # Save final model\n    torch.save(model.cpu(), \"hierarchical_attention_final_model.pt\")\n    print(\"Training completed on all files. Final model saved.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:45:27.186784Z","iopub.execute_input":"2025-04-21T10:45:27.187179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}